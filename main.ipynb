{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj6HLzkCuos6OVjcPIfxC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb3OLK1qUWtW"
      },
      "outputs": [],
      "source": [
        "\"\"\"Untitled84.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1QyswmFpDOafdCi7qQgarEsNtNefHhs-D\n",
        "\"\"\"\n",
        "\n",
        "# https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "\n",
        "# # https://raw.githubusercontent.com/songlab-cal/tape/master/tape/tokenizers.py\n",
        "# !wget https://raw.githubusercontent.com/songlab-cal/tape/master/tape/tokenizers.py\n",
        "\n",
        "import re\n",
        "\n",
        "def smiles_atom_tokenizer (smi):\n",
        "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
        "    regex = re.compile(pattern)\n",
        "    tokens = [token for token in regex.findall(smi)]\n",
        "    return tokens\n",
        "\n",
        "def duplicates(lst, item):\n",
        "    return [i for i, x in enumerate(lst) if x == item]\n",
        "\n",
        "def split_tokens(tokens_to_be_splitted, all_tokens):\n",
        "    if ''.join(tokens_to_be_splitted) in all_tokens:\n",
        "        indexes = duplicates(all_tokens, ''.join(tokens_to_be_splitted))\n",
        "        for k, idx in enumerate(indexes):\n",
        "            all_tokens[idx] = tokens_to_be_splitted[0]\n",
        "            for i in range(1, len(tokens_to_be_splitted)):\n",
        "                all_tokens.insert(idx+i, tokens_to_be_splitted[i])\n",
        "            if k < len(indexes) - 1:\n",
        "                indexes[k+1:] = list(map(lambda x: x+(len(tokens_to_be_splitted) - 1), indexes[k+1:]))\n",
        "def add_symbol(pat):\n",
        "    new_patt = pat[1:].split(',')\n",
        "    ls_del = []\n",
        "    if new_patt[0][0] == '0':\n",
        "        ls_del.append(new_patt[0][0])\n",
        "        new_patt[0] = new_patt[0][1:]\n",
        "\n",
        "    while int(new_patt[0]) > int(new_patt[1]):\n",
        "        ls_del.append(new_patt[0][0])\n",
        "        new_patt[0] = new_patt[0][1:]\n",
        "    result = ['.', ''.join(ls_del), '^', new_patt[0], ',', new_patt[1]]\n",
        "    return result\n",
        "\n",
        "def check_correct_tokenize(iupac,tokens):\n",
        "    if ''.join(tokens).replace('^', '') == iupac:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def iupac_tokenizer(iupac):\n",
        "\n",
        "    pattern = \"\\.\\d+,\\d+|[1-9]{2}\\-[a-z]\\]|[0-9]\\-[a-z]\\]|[1-9]{2}[a-z]|[1-9]{2}'[a-z]|[0-9]'[a-z]|[0-9][a-z]|\\([0-9]\\+\\)|\\([0-9]\\-\\)|\" + \\\n",
        "              \"[1-9]{2}|[0-9]|-|\\s|\\(|\\)|S|R|E|Z|N|C|O|'|\\\"|;|Î»|H|,|\\.|\\[[a-z]{2}\\]|\\[[a-z]\\]|\\[|\\]|\"\n",
        "\n",
        "    alcane = \"methane|methanoyl|methan|ethane|ethanoyl|ethan|propanoyl|propane|propan|propa|butane|butanoyl|butan|buta|pentane|\" + \\\n",
        "             \"pentanoyl|pentan|hexane|hexanoyl|hexan|heptane|heptanoyl|heptan|octane|octanoyl|octan|nonane|nonanoyl|\" + \\\n",
        "             \"nonan|decane|decanoyl|decan|icosane|icosan|cosane|cosan|contane|contan|\"\n",
        "\n",
        "    pristavka_name = \"hydroxide|hydroxyl|hydroxy|hydrate|hydro|cyclo|spiro|iso|\"\n",
        "    pristavka_digit = \"mono|un|bis|bi|dicta|di|tetraza|tetraz|tetra|tetr|pentaza|pentaz|penta|hexaza|\" + \\\n",
        "                      \"hexa|heptaza|hepta|octaza|octa|nonaza|nona|decaza|deca|kis|\"\n",
        "\n",
        "    prefix_alcane = \"methylidene|methyl|ethyl|isopropyl|propyl|isobutyl|sec-butyl|tert-butyl|butyl|pentyl|hexyl|heptyl|octyl|\"\n",
        "    carbon = \"meth|eth|prop|but|pent|hex|hept|oct|non|dec|icosa|icos|cosa|cos|icon|conta|cont|con|heni|hene|hen|hecta|hect|\"\n",
        "\n",
        "    prefix_all = \"benzhydryl|benzoxaza|benzoxaz|benzoxa|benzox|benzo|benzyl|benz|phenacyl|phenanthro|phenyl|phenoxaza|phenoxaz|phenoxy|phenox|phenol|pheno|phen|acetyl|aceto|acet|\" + \\\n",
        "                 \"peroxy|oxido|oxino|oxalo|oxolo|oxocyclo|oxol|oxoc|oxon|oxo|oxy|pyrido|pyrimido|imidazo|naphtho|stiboryl|stibolo|\"\n",
        "\n",
        "    prefix_isotope = \"protio|deuterio|tritio|\"\n",
        "    suffix_isotope = \"protide|\"\n",
        "    prefix_galogen = \"fluoro|fluoranyl|fluoridoyl|fluorido|chloro|chloranyl|chloridoyl|chlorido|bromo|bromanyl|bromidoyl|bromido|iodo|iodanyl|iodidoyl|iodanuidyl|iodido|\"\n",
        "    suffix_galogen = \"fluoride|chloride|chloridic|perchloric|bromide|iodide|iodane|hypoiodous|hypochlorous|\"\n",
        "    prefix_chalcogen = \"phosphonato|phosphoroso|phosphonia|phosphoryl|phosphanyl|arsono|arsanyl|stiba|\"\n",
        "    suffix_chalcogen = \"phosphanium|phosphate|phosphite|phosphane|phosphanide|phosphonamidic|phosphonous|phosphinous|phosphinite|phosphono|arsonic|stibane|\"\n",
        "    prefix_metal = \"alumanyl|gallanyl|stannyl|plumbyl|\"\n",
        "    suffix_metal = \"chromium|stannane|gallane|alumane|aluminane|aluminan|\"\n",
        "    prefix_non_metal = \"tellanyl|germanyl|germyl|\"\n",
        "    suffix_non_metal = \"germane|germa|\"\n",
        "\n",
        "    prefix_sulfur = \"sulfanylidene|sulfinamoyl|sulfonimidoyl|sulfinimidoyl|sulfamoyl|sulfonyl|sulfanyl|sulfinyl|sulfinato|sulfenato|\" + \\\n",
        "                    \"sulfonato|sulfonio|sulfino|sulfono|sulfido|\"\n",
        "    suffix_sulfur = \"sulfonamide|sulfinamide|sulfonamido|sulfonic|sulfamic|sulfinic|sulfuric|thial|thione|thiol|\" + \\\n",
        "                    \"sulfonate|sulfite|sulfate|sulfide|sulfinate|sulfanium|sulfamate|sulfane|sulfo|\"\n",
        "\n",
        "    prefix_nitrogen = \"hydrazono|hydrazino|nitroso|nitrous|nitro|formamido|amino|amido|imino|imido|anilino|anilin|thiocyanato|cyanato|cyano|azido|azanidyl|azanyl|\" + \\\n",
        "                      \"azanide|azanida|azonia|azonio|amidino|nitramido|diazo|\"\n",
        "    suffix_nitrogen = \"ammonium|hydrazide|hydrazine|hydrazin|amine|imine|oxamide|nitramide|formamide|cyanamide|amide|imide|amidine|isocyanide|azanium|\" + \\\n",
        "                      \"thiocyanate|cyanate|cyanic|cyanatidoyl|cyanide|nitrile|nitrite|hydrazonate|\"\n",
        "\n",
        "    suffix_carbon = \"carbonitrile|carboxamide|carbamimidothioate|carbodithioate|carbohydrazonate|carbonimidoyl|carboximidoyl|\" + \\\n",
        "                    \"carbamimidoyl|carbamimidate|carbamimid|carbaldehyde|carbamate|carbothioyl|carboximidothioate|carbonate|\" + \\\n",
        "                    \"carboximidamide|carboximidate|carbamic|carbonochloridate|carbothialdehyde|carbothioate|carbothioic|carbono|carbon|carbo|\" + \\\n",
        "                    \"formate|formic|\"\n",
        "    prefix_carbon = \"carboxylate|carboxylato|carboxylic|carboxy|halocarbonyl|carbamoyl|carbonyl|carbamo|thioformyl|formyl|\"\n",
        "\n",
        "    silicon = \"silanide|silane|silole|silanyl|silyloxy|silylo|silyl|sila|\"\n",
        "    boron = \"boranyl|boranuide|boronamidic|boranuida|boranide|borinic|borate|borane|boran|borono|boron|bora|\"\n",
        "    selenium = \"selanyl|seleno|\"\n",
        "\n",
        "    suffix_all = \"ane|ano|an|ene|enoxy|eno|en|yne|yn|yl|peroxol|peroxo|\" + \\\n",
        "                 \"terephthalate|terephthalic|phthalic|phthalate|oxide|oate|ol|oic|ic|al|ate|ium|one|\"\n",
        "\n",
        "    carbon_trivial = \"naphthalen|naphthal|inden|adamant|fluoren|thiourea|urea|anthracen|acenaphthylen|\" + \\\n",
        "                     \"carbohydrazide|annulen|aniline|acetaldehyde|benzaldehyde|formaldehyde|phthalaldehyde|acephenanthrylen|\" + \\\n",
        "                     \"phenanthren|chrysen|carbanid|chloroform|fulleren|cumen|formonitril|fluoranthen|terephthalaldehyde|azulen|picen|\" + \\\n",
        "                     \"pyren|pleiaden|coronen|tetracen|pentacen|perylen|pentalen|heptalen|cuban|hexacen|oxanthren|ovalen|aceanthrylen|\"\n",
        "\n",
        "    heterocycles = \"indolizin|arsindol|indol|furan|furo|piperazin|pyrrolidin|pyrrolizin|thiophen|thiolo|imidazolidin|imidazol|pyrimidin|pyridin|\" + \\\n",
        "                    \"piperidin|morpholin|pyrazol|pyridazin|oxocinnolin|cinnolin|pyrrol|thiochromen|oxochromen|chromen|quinazolin|phthalazin|quinoxalin|carbazol|xanthen|pyrazin|purin|\" + \\\n",
        "                    \"indazol|naphthyridin|quinolizin|guanidin|pyranthren|pyran|thianthren|thian|acridin|acrido|yohimban|porphyrin|pteridin|tetramin|pentamin|\" + \\\n",
        "                    \"borinin|borino|boriran|borolan|borol|borinan|phenanthridin|quinolin|perimidin|corrin|phenanthrolin|phosphinolin|indacen|silonin|borepin|\"\n",
        "\n",
        "    prefix_heterocycles = \"thiaz|oxaza|oxaz|oxan|oxa|ox|aza|az|thia|thioc|thion|thio|thi|telluro|phospha|phosph|selen|bor|sil|alum|ars|germ|tellur|imid|\"\n",
        "\n",
        "    suffix_heterocycles = \"ir|et|olo|ol|ino|in|ep|oc|on|ec|\"\n",
        "    saturated_unsatured = \"idine|idene|idin|ane|an|ine|in|id|e|\"\n",
        "    pristavka_exception = \"do|trisodium|tris|triacetyl|triamine|triaza|triaz|tria|trityl|tri|o\"\n",
        "\n",
        "    type_ = \"acid|ether|\"\n",
        "    element = \"hydrogen|helium|lithium|beryllium|nitrogen|oxygen|fluorine|neon|sodium|magnesium|aluminum|silicon|\" + \\\n",
        "              \"phosphorus|sulfur|chlorine|argon|potassium|calcium|scandium|titanium|vanadium|chromium|manganese|iron|\" + \\\n",
        "              \"cobalt|nickel|copper|zinc|gallium|germanium|arsenic|selenium|bromine|krypton|rubidium|yttrium|zirconium|\" + \\\n",
        "              \"niobium|molybdenum|technetium|ruthenium|rhodium|palladium|silver|cadmium|indium|antimony|tellurium|iodine|\" + \\\n",
        "              \"xenon|cesium|barium|lanthanum|cerium|praseodymium|neodymium|latinum|promethium|samarium|europium|gadolinium|\" + \\\n",
        "              \"terbium|dysprosium|holmium|erbium|thulium|ytterbium|lutetium|hafnium|tantalum|tungsten|rhenium|osmium|\" + \\\n",
        "              \"iridium|platinum|gold|aurum|mercury|thallium|lead|bismuth|polonium|astatine|radon|francium|radium|actinium|\" + \\\n",
        "              \"thorium|protactinium|uranium|neptunium|plutonium|americium|curium|berkelium|einsteinium|fermium|californium|\" + \\\n",
        "              \"mendelevium|nobelium|lawrencium|rutherfordium|dubnium|seaborgium|bohrium|hassium|meitnerium|tin|\"\n",
        "\n",
        "    other_ions = \"perchlorate|perbromate|periodate|hypofluorite|hypochlorite|hypobromite|hypoiodite|nitrate|silicate|hydride|\"\n",
        "\n",
        "    regex = re.compile(pattern + heterocycles + carbon_trivial + type_ + element + prefix_isotope + other_ions + alcane + pristavka_digit + pristavka_name + prefix_alcane + \\\n",
        "                       carbon + silicon + prefix_nitrogen + prefix_sulfur + prefix_carbon + prefix_metal + prefix_non_metal + prefix_all + prefix_galogen + prefix_chalcogen + \\\n",
        "                       suffix_carbon + suffix_nitrogen + suffix_sulfur + suffix_galogen + suffix_chalcogen + suffix_metal + suffix_non_metal + suffix_all + suffix_heterocycles + \\\n",
        "                       suffix_isotope + boron + selenium  + prefix_heterocycles + saturated_unsatured + pristavka_exception)\n",
        "    tokens = [token for token in regex.findall(iupac)]\n",
        "\n",
        "    split_tokens(['meth', 'ane'], tokens)\n",
        "    split_tokens(['meth', 'an'], tokens)\n",
        "    split_tokens(['eth', 'ane'], tokens)\n",
        "    split_tokens(['eth', 'an'], tokens)\n",
        "    split_tokens(['prop', 'ane'], tokens)\n",
        "    split_tokens(['prop', 'an'], tokens)\n",
        "    split_tokens(['but', 'ane'], tokens)\n",
        "    split_tokens(['but', 'an'], tokens)\n",
        "    split_tokens(['pent', 'ane'], tokens)\n",
        "    split_tokens(['pent', 'an'], tokens)\n",
        "    split_tokens(['hex', 'ane'], tokens)\n",
        "    split_tokens(['hex', 'an'], tokens)\n",
        "    split_tokens(['hept', 'ane'], tokens)\n",
        "    split_tokens(['hept', 'an'], tokens)\n",
        "    split_tokens(['oct', 'ane'], tokens)\n",
        "    split_tokens(['oct', 'an'], tokens)\n",
        "    split_tokens(['non', 'ane'], tokens)\n",
        "    split_tokens(['non', 'an'], tokens)\n",
        "    split_tokens(['dec', 'ane'], tokens)\n",
        "    split_tokens(['dec', 'an'], tokens)\n",
        "    split_tokens(['cos', 'ane'], tokens)\n",
        "    split_tokens(['cos', 'an'], tokens)\n",
        "    split_tokens(['cont', 'ane'], tokens)\n",
        "    split_tokens(['cont', 'an'], tokens)\n",
        "    split_tokens(['icos', 'ane'], tokens)\n",
        "    split_tokens(['icos', 'an'], tokens)\n",
        "\n",
        "    split_tokens(['thi', 'az'], tokens)\n",
        "    split_tokens(['thi', 'oc'], tokens)\n",
        "    split_tokens(['thi', 'on'], tokens)\n",
        "    split_tokens(['benz', 'ox'], tokens)\n",
        "    split_tokens(['benz', 'oxa'], tokens)\n",
        "    split_tokens(['benz', 'ox', 'az'], tokens)\n",
        "    split_tokens(['benz', 'ox', 'aza'], tokens)\n",
        "    split_tokens(['phen', 'ox'], tokens)\n",
        "    split_tokens(['phen', 'oxy'], tokens)\n",
        "    split_tokens(['phen', 'oxa'], tokens)\n",
        "    split_tokens(['phen', 'ox', 'az'], tokens)\n",
        "    split_tokens(['phen', 'ox', 'aza'], tokens)\n",
        "    split_tokens(['phen', 'ol'], tokens)\n",
        "    split_tokens(['en', 'oxy'], tokens)\n",
        "    split_tokens(['ox', 'az'], tokens)\n",
        "    split_tokens(['ox', 'aza'], tokens)\n",
        "    split_tokens(['tri', 'az'], tokens)\n",
        "    split_tokens(['tri', 'amine'], tokens)\n",
        "    split_tokens(['tri', 'acetyl'], tokens)\n",
        "    split_tokens(['ox', 'ol'], tokens)\n",
        "    split_tokens(['ox', 'olo'], tokens)\n",
        "    split_tokens(['ox', 'an'], tokens)\n",
        "    split_tokens(['ox', 'oc'], tokens)\n",
        "    split_tokens(['ox', 'on'], tokens)\n",
        "    split_tokens(['tri', 'az'], tokens)\n",
        "    split_tokens(['tri', 'aza'], tokens)\n",
        "    split_tokens(['tri', 'sodium'], tokens)\n",
        "    split_tokens(['tetr', 'az'], tokens)\n",
        "    split_tokens(['tetr', 'aza'], tokens)\n",
        "    split_tokens(['pent', 'az'], tokens)\n",
        "    split_tokens(['pent', 'aza'], tokens)\n",
        "    split_tokens(['hex', 'aza'], tokens)\n",
        "    split_tokens(['hept', 'aza'], tokens)\n",
        "    split_tokens(['oct', 'aza'], tokens)\n",
        "    split_tokens(['non', 'aza'], tokens)\n",
        "    split_tokens(['dec', 'aza'], tokens)\n",
        "    split_tokens(['oxo', 'chromen'], tokens)\n",
        "    split_tokens(['oxo', 'cinnolin'], tokens)\n",
        "    split_tokens(['oxo', 'cyclo'], tokens)\n",
        "    split_tokens(['thio', 'chromen'], tokens)\n",
        "    split_tokens(['thio', 'cyanato'], tokens)\n",
        "\n",
        "    if (len(re.findall(re.compile('[0-9]{2}\\-[a-z]\\]'), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile('[0-9]{2}\\-[a-z]\\]'), tok):\n",
        "                tokens[i] = tok[:2]\n",
        "                tokens.insert(i+1,tok[2])\n",
        "                tokens.insert(i+2,tok[3])\n",
        "                tokens.insert(i+3,tok[4])\n",
        "\n",
        "    if (len(re.findall(re.compile('[0-9]\\-[a-z]\\]'), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile('[0-9]\\-[a-z]\\]'), tok):\n",
        "                tokens[i] = tok[:1]\n",
        "                tokens.insert(i+1,tok[1])\n",
        "                tokens.insert(i+2,tok[2])\n",
        "                tokens.insert(i+3,tok[3])\n",
        "\n",
        "    if (len(re.findall(re.compile('\\[[a-z]{2}\\]'), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile('\\[[a-z]{2}\\]'), tok):\n",
        "                tokens[i] = tok[0]\n",
        "                tokens.insert(i+1,tok[1])\n",
        "                tokens.insert(i+2,tok[2])\n",
        "                tokens.insert(i+3,tok[3])\n",
        "\n",
        "    if (len(re.findall(re.compile('\\[[a-z]\\]'), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile('\\[[a-z]\\]'), tok):\n",
        "                tokens[i] = tok[0]\n",
        "                tokens.insert(i+1,tok[1])\n",
        "                tokens.insert(i+2,tok[2])\n",
        "\n",
        "    if (len(re.findall(re.compile(\"[0-9]{2}'[a-z]\"), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile(\"[0-9]{2}'[a-z]\"), tok):\n",
        "                tokens[i] = tok[:2]\n",
        "                tokens.insert(i+1,tok[2])\n",
        "                tokens.insert(i+2,tok[3])\n",
        "\n",
        "    if (len(re.findall(re.compile(\"[0-9]'[a-z]\"), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile(\"[0-9]'[a-z]\"), tok):\n",
        "                tokens[i] = tok[0]\n",
        "                tokens.insert(i+1,tok[1])\n",
        "                tokens.insert(i+2,tok[2])\n",
        "\n",
        "    if (len(re.findall(re.compile(\"[0-9]{2}[a-z]\"), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile(\"[0-9]{2}[a-z]\"), tok):\n",
        "                tokens[i] = tok[:2]\n",
        "                tokens.insert(i+1,tok[2])\n",
        "\n",
        "    if (len(re.findall(re.compile(\"[0-9][a-z]\"), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile(\"[0-9][a-z]\"), tok):\n",
        "                tokens[i] = tok[0]\n",
        "                tokens.insert(i+1,tok[1])\n",
        "\n",
        "    if (len(re.findall(re.compile(\"\\.\\d+,\\d+\"), ''.join(tokens))) > 0):\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if re.findall(re.compile(\"\\.\\d+,\\d+\"), tok):\n",
        "                result = add_symbol(tok)\n",
        "                tokens[i] = result[0]\n",
        "                for k, res in enumerate(result[1:], start=1):\n",
        "                    tokens.insert(i+k,res)\n",
        "\n",
        "    if check_correct_tokenize(iupac, tokens) == True:\n",
        "        return tokens\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\"\"\"## Model Definition\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pylab\n",
        "\n",
        "CHANNELS = [64, 64, 128, 256, 512]\n",
        "RES_NUMBER = 4\n",
        "EMBEDING_DIM =1024\n",
        "HEADS = 16\n",
        "DECODER_NUM = 1\n",
        "\n",
        "class ResNetBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, channel1, channel2, name):\n",
        "    super().__init__(name=name)\n",
        "    self.Conv2D = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(channel1, (3,3), padding=\"same\", strides = 2),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Conv2D(channel2, (3,3), padding=\"same\"),\n",
        "        tf.keras.layers.BatchNormalization()\n",
        "    ])\n",
        "    self.project =  tf.keras.layers.Conv2D(channel2, (1,1), name=\"image_projection\", strides = 2)\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "\n",
        "  def call(self, images):\n",
        "    x = self.Conv2D(images)\n",
        "    images = self.project(images)\n",
        "    return self.relu(x+images)\n",
        "\n",
        "class PosEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, max_length):\n",
        "    super().__init__()\n",
        "    self.max_length = max_length\n",
        "    self.Embeding =  tf.keras.layers.Embedding(max_length, EMBEDING_DIM, input_length=max_length)\n",
        "\n",
        "  def call(self, x):\n",
        "    tags = tf.range(self.max_length)\n",
        "    return self.Embeding(tags[tf.newaxis,:]) + x\n",
        "\n",
        "\n",
        "class ImageEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv2D_7 = tf.keras.layers.Conv2D(CHANNELS[0], (7, 7), padding=\"same\", strides = 2)\n",
        "    self.pooling = tf.keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)\n",
        "    self.ResBlocks = []\n",
        "    for i in range(RES_NUMBER):\n",
        "      self.ResBlocks.append(ResNetBlock(CHANNELS[i+1], CHANNELS[i+1]*2, name=f\"resblock_{i}\"))\n",
        "    self.flatten = tf.keras.layers.Reshape((49,1024))\n",
        "    self.img_pos_encoding = PosEncoding(49)\n",
        "    self.attention = tf.keras.layers.Attention()\n",
        "\n",
        "  def call(self, images):\n",
        "    images = self.conv2D_7(images)\n",
        "    images = self.pooling(images)\n",
        "    for block in self.ResBlocks:\n",
        "      images = block(images)\n",
        "    images = self.flatten(images)\n",
        "    images = self.img_pos_encoding(images)\n",
        "    return images #self.attention([images, images, images])\n",
        "\n",
        "  # def model(self):\n",
        "  #     x = tf.keras.layers.Input(shape=(400, 400, 3))\n",
        "  #     return tf.keras.Model(inputs=[x], outputs=self.call(x))\n",
        "  # https://stackoverflow.com/questions/55235212/model-summary-cant-print-output-shape-while-using-subclass-model\n",
        "\n",
        "class AddNorm(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, y):\n",
        "    return self.norm(x+y)\n",
        "\n",
        "\n",
        "class TextEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_token, max_length, first_layer, name=\"\"):\n",
        "    if name == \"\":\n",
        "      super().__init__()\n",
        "    else:\n",
        "      super().__init__(name=name)\n",
        "    self.first_layer = first_layer\n",
        "    self.num_token, self.max_length = num_token, max_length\n",
        "    self.pos_encoding = PosEncoding(max_length) #added one above already\n",
        "    self.Embeding =  tf.keras.layers.Embedding(num_token, EMBEDING_DIM, input_length=max_length)\n",
        "    self.attention = tf.keras.layers.MultiHeadAttention(HEADS, 1024, dropout=0.3)\n",
        "    self.add_and_norm = AddNorm()\n",
        "\n",
        "  def call(self, x):\n",
        "    if self.first_layer:\n",
        "      x = self.Embeding(x)\n",
        "      x = self.pos_encoding(x)\n",
        "    attention_score = self.attention(x, x, use_causal_mask=1)\n",
        "    return self.add_and_norm(attention_score, x)\n",
        "\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_token):\n",
        "    super().__init__()\n",
        "    self.MultiHeadAttention = tf.keras.layers.MultiHeadAttention(HEADS, 1024, dropout=0.3)\n",
        "\n",
        "  def call(self, image_features, texts):\n",
        "    return self.MultiHeadAttention(value=image_features, query=texts)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_token, max_length):\n",
        "    super().__init__()\n",
        "    # self.TextEncoder = TextEncoder(num_token, max_length)\n",
        "\n",
        "    # input = tf.keras.layers.Input(shape=(400, 400, 3))\n",
        "    self.ImageEncoder = ImageEncoder()\n",
        "    #  tf.keras.Model(inputs=[input], outputs=ImageEncoder().call(input))\n",
        "\n",
        "    # # input2 = tf.keras.layers.Input(shape=(100, 500))\n",
        "    # #tf.keras.Model(inputs=[input2], outputs=TextEncoder(100, 500).call(input2))\n",
        "    # input2 = tf.keras.layers.Input(shape=(500))\n",
        "    # self.TextEncoder = tf.keras.Model(inputs=[input2], outputs=TextEncoder(100, 500).call(input2))\n",
        "\n",
        "    self.CrossAttentions = []\n",
        "    self.add_and_norm1s = []\n",
        "    self.add_and_norm2s = []\n",
        "    self.MLPs = []\n",
        "    self.TextEncoders = []\n",
        "\n",
        "\n",
        "    for i in range(DECODER_NUM):\n",
        "      self.TextEncoders.append(TextEncoder(num_token, max_length, i==0))\n",
        "      self.CrossAttentions.append(CrossAttention(num_token))\n",
        "      self.add_and_norm1s.append(AddNorm())\n",
        "      self.add_and_norm2s.append(AddNorm())\n",
        "      self.MLPs.append(tf.keras.Sequential([\n",
        "          tf.keras.layers.Dense(2048, activation=\"relu\"),\n",
        "          tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
        "          tf.keras.layers.Dropout(0.1)], name=f\"MLP_{i}\"))\n",
        "\n",
        "    self.DenseWithSoftmax = tf.keras.Sequential(\n",
        "        [tf.keras.layers.Dense(num_token),\n",
        "        tf.keras.layers.Dense(num_token, activation=\"softmax\")], name=\"linear_and_softmax\")\n",
        "\n",
        "  def call(self, images, texts):\n",
        "    images = self.ImageEncoder(images)\n",
        "    for i in range(DECODER_NUM):\n",
        "      texts = self.TextEncoders[i](texts)\n",
        "      features = self.CrossAttentions[i](images, texts)\n",
        "      features = self.add_and_norm1s[i](features, texts)\n",
        "      texts = self.add_and_norm2s[i](self.MLPs[i](features), features)\n",
        "      # text = self.add_and_norm2s[i](self.MLPs[i](features), features)\n",
        "    return self.DenseWithSoftmax(texts)\n",
        "\n",
        "model = Decoder(100, 500)\n",
        "input1 = tf.keras.layers.Input(shape=(400, 400, 3))\n",
        "input2 = tf.keras.layers.Input(shape=(500)) #input is NOT onehot\n",
        "model =  tf.keras.Model(inputs=[input1, input2], outputs=model.call(input1, input2))\n",
        "model.summary(expand_nested=1)\n",
        "#tf.keras.utils.plot_model(model)\n",
        "\n",
        "#tf.keras.utils.plot_model(model, expand_nested=1, rankdir=\"LR\") #https://stackoverflow.com/questions/62337528/how-to-plot-machine-learning-model-horizontally\n",
        "\n",
        "#itf.keras.utils.plot_model(model) #https://stackoverflow.com/questions/62337528/how-to-plot-machine-learning-model-horizontally\n",
        "\n",
        "\"\"\"## Data Pre-processing\n",
        "\n",
        "### Text\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\"\"\"\n",
        "if IN_COLAB:\n",
        "  if not \"80k\" in \"\".join(os.listdir(\"/home/wg25r/colab\")):\n",
        "    os.system(\"cp drive/MyDrive/80k.zip .\")\n",
        "    os.system(\"cp drive/MyDrive/80k.csv .\")\n",
        "    os.system(\"unzip 80k.zip\")\n",
        "else:\n",
        "    if not \"80k\" in \"\".join(os.listdir(\"/home/wg25r/colab\")):\n",
        "      os.system(\"wget file.weasoft.com/80k.zip\")\n",
        "      os.system(\"wget http://file.weasoft.com/80k.csv\")\n",
        "      os.system(\"unzip 80k.zip\")\n",
        "\"\"\"\n",
        "import os\n",
        "ids = [i.split(\"_\")[0] for i in os.listdir(\"/arc/project/st-dushan20-1/rendered/rendered\")]\n",
        "\n",
        "with open(\"ids.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(list(set(ids))))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "csv = pd.read_csv(\"80k.csv\")\n",
        "\n",
        "csv.columns\n",
        "#kk fucking eifaxian iupacname\n",
        "\n",
        "def valid_iupac(i):\n",
        "  try:\n",
        "    return (iupac_tokenizer(i)!=None)\n",
        "  except:\n",
        "    return (False)\n",
        "\n",
        "cids = csv[\"cid\"]\n",
        "\n",
        "Ys = {}\n",
        "invalid_cid = []\n",
        "for i in cids.values:\n",
        "  try:\n",
        "    tmp = iupac_tokenizer(csv[csv[\"cid\"] == i][\"iupacname\"].values[0])\n",
        "    if not tmp == None:\n",
        "      Ys[i] = tmp\n",
        "    else:\n",
        "      invalid_cid.append(i)\n",
        "\n",
        "  except:\n",
        "    invalid_cid.append(i)\n",
        "\n",
        "len(invalid_cid)\n",
        "\n",
        "Ys_in = {}\n",
        "Ys_out = {}\n",
        "tokens = []\n",
        "max_length = 0\n",
        "for i in Ys.keys():\n",
        "  # Ys_in[i] = \"ðŸ˜˜\" + Ys[i]\n",
        "  # Ys_out[i] = Ys[i] + \"ðŸ˜¢\"\n",
        "  tokens.extend(Ys[i])\n",
        "  max_length = max(max_length, len(Ys[i]))\n",
        "\n",
        "\n",
        "tokens = [\"ðŸœ\",\"ðŸ˜˜\",\"ðŸ˜¢\"]+list(set(tokens))\n",
        "\n",
        "def str_to_tokens(a: str):\n",
        "  return [tokens.index(i) for i in a]\n",
        "\n",
        "for i in Ys.keys():\n",
        "  t = str_to_tokens(Ys[i])\n",
        "  # Ys_in[i] = [1] + t + ([0] * (max_length-len(t)))\n",
        "  Ys_in[i] =  ([0] * (max_length-len(t))) + [1] + t\n",
        "  Ys_out[i] = ([0] * (max_length-len(t))) + t + [2]\n",
        "max_length += 1\n",
        "\n",
        "len(Ys_in[list(Ys.keys())[1]]), max_length\n",
        "\n",
        "\"\"\"### Image\"\"\"\n",
        "\n",
        "# No processing at this point\n",
        "\n",
        "invalid_cid = set(invalid_cid)\n",
        "\n",
        "\"\"\"## Pipeline\"\"\"\n",
        "\n",
        "# https://medium.com/analytics-vidhya/write-your-own-custom-data-generator-for-tensorflow-keras-1252b64e41c3\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "    def __init__(self, batch_size, training):\n",
        "      self.batch_size = batch_size\n",
        "      self.dir_ = os.listdir(\"/arc/project/st-dushan20-1/rendered/rendered\")\n",
        "      self.dir = []\n",
        "      for i in self.dir_:\n",
        "        if not int(i.split(\"_\")[0]) in invalid_cid:\n",
        "          self.dir.append(i)\n",
        "      np.random.shuffle(self.dir)\n",
        "      self.ids = np.array(set([i.split(\"_\")[0] for i in self.dir]))\n",
        "      self.offset = self.dir[:-2048] if training else self.dir[2048:]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.dir)\n",
        "\n",
        "    def __getitem__(self, index_):\n",
        "        start_index = index_ * self.batch_size\n",
        "        Xs_img = []\n",
        "        Xs_text = []\n",
        "        y = [] #This is slow, rewrite later\n",
        "        for _ in range(self.batch_size):\n",
        "          index = start_index + _\n",
        "          try:\n",
        "            id = int(self.dir[index].split(\"_\")[0])\n",
        "          except:\n",
        "            break\n",
        "          index = start_index + _\n",
        "          img = np.array(Image.open(f\"/arc/project/st-dushan20-1/rendered/rendered/{self.dir[index]}\").rotate(np.random.uniform(0,360), expand = 1).resize((400,400)), dtype=\"float32\")\n",
        "          np.random.uniform(size=img.shape)*20\n",
        "          img += noise\n",
        "          Xs_img.append(img)\n",
        "          Xs_text.append(Ys_in[id])\n",
        "          y.append(Ys_out[id])\n",
        "        return [np.array(Xs_img), np.array(Xs_text)], np.array(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dir)//self.batch_size//30\n",
        "\n",
        "img = np.array(Image.open(f\"/arc/project/st-dushan20-1/rendered/rendered/10011914_0.jpg\").rotate(np.random.uniform(0,180)), dtype=\"float32\")\n",
        "noise = np.random.uniform(size=img.shape)*20\n",
        "#pylab.imshow(np.array(noise+img, dtype=\"uint8\"))\n",
        "\n",
        "for i in os.listdir(\"/arc/project/st-dushan20-1/rendered/rendered\"):\n",
        "  if not \"_\" in i:\n",
        "    print(\"P\")\n",
        "\n",
        "gen = CustomDataGen(256, 1)\n",
        "gen.__getitem__(1)\n",
        "\n",
        "\"\"\"## Training\"\"\"\n",
        "\n",
        "model = Decoder(len(tokens), max_length)\n",
        "#input1 = tf.keras.layers.Input(shape=(400, 400, 3))\n",
        "#input2 = tf.keras.layers.Input(shape=(max_length)) #input is NOT onehot\n",
        "#model =  tf.keras.Model(inputs=[input1, input2], outputs=model.call(input1, input2))\n",
        "#model.summary(expand_nested=1)\n",
        "\n",
        "def predict(model, img_id):\n",
        "    seq = np.array([1]+[0]*(max_length-1))\n",
        "    pred = model([np.array([np.array(Image.open(f\"/arc/project/st-dushan20-1/rendered/rendered/{img_id}_0.jpg\"), dtype=\"float32\")]), np.array([seq])])[0]\n",
        "    return \"\".join([tokens[tf.argmax(i)] for i in pred])\n",
        "\n",
        "img_id = 10011914\n",
        "print(predict(model, 10011914))\n",
        "#pylab.imshow(Image.open(f\"/arc/project/st-dushan20-1/rendered/rendered/{10011914}_1.jpg\"))\n",
        "\n",
        "# https://www.tensorflow.org/tutorials/text/image_captioning\n",
        "# https://keras.io/api/losses/probabilistic_losses/#categorical_crossentropy-function\n",
        "# https://arxiv.org/pdf/1706.03762.pdf\n",
        "# https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, label_smoothing=0.1)\n",
        "\n",
        "token_count = len(tokens)\n",
        "def masked_loss(labels, preds):\n",
        "  loss = cce(tf.one_hot(labels, token_count), preds)\n",
        "  mask = tf.cast(labels != 0, dtype=loss.dtype)\n",
        "  return tf.math.reduce_sum(loss*mask)/tf.math.reduce_sum(mask)\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  pred = tf.cast(tf.argmax(preds, axis=-1), tf.float32)\n",
        "  mask = tf.cast(labels != 0, tf.float32)\n",
        "  labels = tf.cast(labels, tf.float32)\n",
        "  match_case = tf.cast(labels == pred, tf.float32)\n",
        "  return tf.math.reduce_sum(match_case*mask)/tf.math.reduce_sum(mask)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "          loss=masked_loss,metrics=[masked_acc])\n",
        "\n",
        "class my_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    old_lr = self.model.optimizer.lr.read_value()\n",
        "    new_lr = old_lr * 0.95\n",
        "    print(\"\\nepoch: {}   lr: {} -> {}\".format(epoch, old_lr, new_lr))\n",
        "    print(predict(self.model, 10011914))\n",
        "    self.model.optimizer.lr.assign(new_lr)\n",
        "\n",
        "  # def on_batch_end(self, batch, logs=None):\n",
        "  #   old_lr = self.model.optimizer.lr.read_value()\n",
        "  #   new_lr = old_lr * 0.999995\n",
        "  #   self.model.optimizer.lr.assign(new_lr)\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "train = CustomDataGen(64, 1)\n",
        "val = CustomDataGen(64, 0)\n",
        "model.fit(train, validation_data=val, epochs=5, callbacks=[my_callback(), tensorboard_callback])"
      ]
    }
  ]
}