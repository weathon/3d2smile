{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/Test_SMILES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnyVcrxlz1Rc"
      },
      "outputs": [],
      "source": [
        "# !wget http://file.weasoft.com/main.pb -nv\n",
        "# !wget http://file.weasoft.com/80k.csv -nv\n",
        "# !wget http://file.weasoft.com/swin_transform_focalloss.pth -nv\n",
        "# !wget http://file.weasoft.com/80k.zip\n",
        "# !unzip -q 80k.zip\n",
        "# !pip3 install torch_xla\n",
        "# !pip3 install deepsmiles yacs tqdm\n",
        "# !git clone https://github.com/suanfaxiaohuo/SwinOCSR.git\n",
        "# !pip install focal_loss_torch wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rovlreP90bP5"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import pylab\n",
        "import pandas\n",
        "import sys\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KA3vnh8U1Nkd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    HOME_DIR = \"/content\"\n",
        "else:\n",
        "    HOME_DIR = \"/arc/burst/st-dushan20-1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "au-7ZSY81Slk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# From the SwinOCSR\n",
        "import sys\n",
        "sys.path.append(\"/content/SwinOCSR/model/Swin-transformer-focalloss\")\n",
        "sys.path.append(\"./SwinOCSR/model/\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import deepsmiles\n",
        "from typing import Any, cast, Callable, List, Tuple, Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from pre_transformer import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BL5Wdj831EqV"
      },
      "outputs": [],
      "source": [
        "# os.environ['PJRT_DEVICE'] = 'TPU' #That is why\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.runtime as xr\n",
        "# device = xm.xla_device()\n",
        "try:\n",
        "  5/0\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xmc\n",
        "  device = xm.xla_device()\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda:0')\n",
        "  else:\n",
        "      device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWiFh9bA1wUV",
        "outputId": "081ced70-b6bc-4130-c9b9-e00217799457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> ./swin_transform_focalloss.pth <=====\n",
            "Decoder: <All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['cid', 'cmpdname', 'cmpdsynonym', 'mw', 'mf', 'polararea', 'complexity',\n",
              "       'xlogp', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'inchi',\n",
              "       'isosmiles', 'canonicalsmiles', 'inchikey', 'iupacname', 'exactmass',\n",
              "       'monoisotopicmass', 'charge', 'covalentunitcnt', 'isotopeatomcnt',\n",
              "       'totalatomstereocnt', 'definedatomstereocnt', 'undefinedatomstereocnt',\n",
              "       'totalbondstereocnt', 'definedbondstereocnt', 'undefinedbondstereocnt',\n",
              "       'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'neighbortype', 'meshheadings',\n",
              "       'annothits', 'annothitcnt', 'aids', 'cidcdate', 'sidsrcname', 'depcatg',\n",
              "       'annotation'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "class FocalLossModelInference:\n",
        "    \"\"\"\n",
        "    Inference Class\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Load dictionary that maps tokens to integers\n",
        "        word_map_path = './SwinOCSR/Data/500wan/500wan_shuffle_DeepSMILES_word_map'\n",
        "        self.word_map = torch.load(word_map_path)\n",
        "        self.inv_word_map = {v: k for k, v in self.word_map.items()}\n",
        "\n",
        "        # Define device, load models and weights\n",
        "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # self.args, config = self.get_inference_config()\n",
        "        # self.encoder = build_model(config, tag=False)\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.load_checkpoint(\"./swin_transform_focalloss.pth\")\n",
        "        self.decoder = self.decoder.to(self.dev).eval()\n",
        "        # self.encoder = self.encoder.to(self.dev).eval()\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load checkpoint and update encoder and decoder accordingly\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path (str): path of checkpoint file\n",
        "        \"\"\"\n",
        "        print(f\"=====> {checkpoint_path} <=====\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        # encoder_msg = self.encoder.load_state_dict(checkpoint['encoder'],\n",
        "        #                                            strict=False)\n",
        "        decoder_msg = self.decoder.load_state_dict(checkpoint['decoder'],\n",
        "                                                   strict=False)\n",
        "        # print(f\"Encoder: {encoder_msg}\")\n",
        "        print(f\"Decoder: {decoder_msg}\")\n",
        "        del checkpoint\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def build_decoder(self):\n",
        "        \"\"\"\n",
        "        This method builds the Transformer decoder and returns it\n",
        "        \"\"\"\n",
        "        self.decoder_dim = 256  # dimension of decoder RNN\n",
        "        self.ff_dim = 2048\n",
        "        self.num_head = 8\n",
        "        self.dropout = 0.1\n",
        "        self.encoder_num_layer = 6\n",
        "        self.decoder_num_layer = 6\n",
        "        self.max_len = 277\n",
        "        self.decoder_lr = 5e-4\n",
        "        self.best_acc = 0.\n",
        "        return Transformer(dim=self.decoder_dim,\n",
        "                           ff_dim=self.ff_dim,\n",
        "                           num_head=self.num_head,\n",
        "                           encoder_num_layer=self.encoder_num_layer,\n",
        "                           decoder_num_layer=self.decoder_num_layer,\n",
        "                           vocab_size=len(self.word_map),\n",
        "                           max_len=self.max_len,\n",
        "                           drop_rate=self.dropout,\n",
        "                           tag=False)\n",
        "transformer_ = FocalLossModelInference()\n",
        "transformer = transformer_.build_decoder().decoder\n",
        "\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "\n",
        "\n",
        "def str_to_vector(s: str)->list:\n",
        "  return [transformer_.word_map[i] for i in converter.encode(s)]\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "csv = pd.read_csv(\"./80k.csv\")\n",
        "cids = csv[\"cid\"]\n",
        "csv.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L9d_nQJ2V_m",
        "outputId": "f70f4111-dc3e-4d12-f71a-ce600bb6bfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOHH\n"
          ]
        }
      ],
      "source": [
        "Ys = {}\n",
        "invalid_cids = []\n",
        "for i in cids.values:\n",
        "    tmp = str_to_vector(csv[csv[\"cid\"] == i][\"canonicalsmiles\"].values[0])\n",
        "    if not tmp == None:\n",
        "      Ys[i] = tmp\n",
        "    else:\n",
        "      invalid_cids.append(i)\n",
        "\n",
        "print(\"OOHH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vt1J6ydd6cht"
      },
      "outputs": [],
      "source": [
        "from focal_loss.focal_loss import FocalLoss\n",
        "m = torch.nn.Softmax(dim=-1)\n",
        "lf = FocalLoss(gamma=2, ignore_index=0)#torch.nn.CrossEntropyLoss(label_smoothing=0.1, reduction=\"none\")\n",
        "def loss_fn(pred, truth):\n",
        "  pred = m(pred)\n",
        "  l = lf(pred, truth)\n",
        "  return l\n",
        "\n",
        "\n",
        "def mask_acc(pred, truth):\n",
        "    pred = torch.argmax(pred, -1)\n",
        "    mask = truth != 0\n",
        "    match_case = truth == pred\n",
        "    return torch.sum(mask*match_case)/torch.sum(mask)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "files = os.listdir(f\"./rendered/\")\n",
        "files = [i for i in files if len(i)<=40]\n",
        "import multiprocessing, threading\n",
        "#https://stackoverflow.com/questions/48822463/how-to-use-pytorch-multiprocessing\n",
        "from torch.multiprocessing import Process, Queue, Pool\n",
        "import time\n",
        "buffer = Queue(maxsize=10) #need maxsize=10, otherwise put will also block\n",
        "start_index = 0\n",
        "import cv2\n",
        "def process_single(arg):\n",
        "    # print(arg)\n",
        "    _, start_index = arg\n",
        "    index = start_index + _\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    img = np.array(Image.open(f\"{HOME_DIR}/rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "    return img, [77] + Ys[id], Ys[id] + [78]\n",
        "\n",
        "#pool = Pool()\n",
        "\n",
        "\n",
        "#read all images\n",
        "images_warehouse = []\n",
        "index = 0\n",
        "import time\n",
        "t0 = time.time()\n",
        "import os, psutil\n",
        "process = psutil.Process()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LPAoMbiFdb5"
      },
      "source": [
        "# reset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "metadata": {
        "id": "Y5ta9TW7ou_5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "DwuU7Rrp53Sw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
        "\n",
        "try:\n",
        "  model.cpu()\n",
        "  gen.cpu()\n",
        "  del model, gen\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "except:\n",
        "  pass\n",
        "example_in = Image.open(\"./rendered/6912034_0.jpg\")\n",
        "example_out = csv[csv[\"cid\"]==6912034][\"canonicalsmiles\"].values[0]\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "example_out = str_to_vector(example_out)\n",
        "\n",
        "\n",
        "torch.permute(torch.tensor(np.expand_dims(example_in, 0).astype(\"float32\")), (0,3,1,2)).size()\n",
        "\n",
        "\n",
        "t = torch.randn(1,64,400,400)\n",
        "torch.flatten(t)\n",
        "torch.flatten(t, start_dim=2,end_dim=3).shape\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "import numpy as np\n",
        "def positional_encoding_1d(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return pos_encoding\n",
        "\n",
        "\n",
        "def positional_encoding_2d(row, col, d_model):\n",
        "    assert d_model % 2 == 0\n",
        "    row_pos = np.repeat(np.arange(row), col)[:, np.newaxis]\n",
        "    col_pos = np.repeat(np.expand_dims(np.arange(col), 0), row, axis=0).reshape(-1, 1)\n",
        "\n",
        "    angle_rads_row = get_angles(\n",
        "        row_pos, np.arange(d_model // 2)[np.newaxis, :], d_model // 2\n",
        "    )\n",
        "    angle_rads_col = get_angles(\n",
        "        col_pos, np.arange(d_model // 2)[np.newaxis, :], d_model // 2\n",
        "    )\n",
        "\n",
        "    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
        "    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
        "    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
        "    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
        "    pos_encoding = np.concatenate([angle_rads_row, angle_rads_col], axis=1)[\n",
        "        np.newaxis, ...\n",
        "    ]\n",
        "    return pos_encoding\n",
        "\n",
        "eff = torchvision.models.efficientnet_v2_l(weights='DEFAULT')\n",
        "mynet = eff.features\n",
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eff = mynet.to(device)\n",
        "    self.projection = torch.nn.Linear(1280,256).to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    features = torch.flatten(features, start_dim=2, end_dim=3)\n",
        "    features = torch.permute(features, (0, 2, 1))\n",
        "    return self.projection(features)\n",
        "\n",
        "\n",
        "\n",
        "encoder = ImageEncoder()\n",
        "\n",
        "for p in transformer.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "\n",
        "# In[45]:\n",
        "# from another paper\n",
        "def pad_pack(sequences):\n",
        "    maxlen = max(map(len, sequences))\n",
        "    batch = torch.LongTensor(len(sequences),maxlen).fill_(0)\n",
        "    for i,x in enumerate(sequences):\n",
        "        batch[i,:len(x)] = torch.LongTensor(x)\n",
        "    return batch, maxlen\n",
        "\n",
        "# https://github.com/suanfaxiaohuo/SwinOCSR/blob/main/model/Swin-transformer-focalloss/pre_transformer.py#L95\n",
        "def triangle_mask(size):\n",
        "    mask = 1- np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
        "    mask = torch.autograd.Variable(torch.from_numpy(mask))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in, xmask):\n",
        "    #padded_text, maxlen = pad_pack(text_in)\n",
        "    #padded_text = padded_text.to(device)\n",
        "    # image = torch.permute(torch.tensor(np.array(image)), (0,3,1,2))\n",
        "    image_feature = self.encoder(image)\n",
        "    out = self.decoder(text_in, image_feature, x_mask=xmask)#triangle_mask(maxlen).to(device))\n",
        "    return out\n",
        "\n",
        "# In[46]:\n",
        "\n",
        "def softmax(x):\n",
        "        t = np.exp(x)\n",
        "        return t/np.sum(t)\n",
        "\n",
        "softmax_1 = torch.nn.Softmax(dim=-1)\n",
        "class SMILESGenerator(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in_, max_len, beam=1): #just changed beam=1 and it runs so beam cannot be random number???  yeah it has to been factor of 676, which is 2 2 13 13 but original paper did not use beam search\n",
        "    image_feature = self.encoder(image)\n",
        "    top_n = list([list(i) for i in torch.tensor(text_in_).repeat(beam,1).detach().cpu().numpy()])\n",
        "    top_n_conf = torch.tensor([1] * beam).to(device)\n",
        "    for i in range(max_len):\n",
        "      # print(top_n)\n",
        "      padded_text, l = pad_pack(top_n)\n",
        "      padded_text = padded_text.to(device)\n",
        "      out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(l).to(device))\n",
        "      out = out[:,-1,:]\n",
        "      # next = torch.topk(out, beam, dim=-1) it is not top k it is sort since we need all of them?\n",
        "      # sorted = torch.sort(out, -1)\n",
        "      #i just relized i do not need to sort, i can mutlipy and then top k\n",
        "      # t = torch.exp(out)\n",
        "      out2 = torch.broadcast_to(top_n_conf, (out.shape[1], beam)).T * softmax_1(out)\n",
        "      # since number of chars are more than beams, so we just do a argmax or top k then topk then we are good?\n",
        "      # next = torch.topk(out, beam, dim=-1)\n",
        "      next = torch.topk(torch.flatten(out2), beam)\n",
        "      top_n_conf = next.values\n",
        "      top_n_ = []\n",
        "      indices2d = np.array(np.unravel_index(next.indices.cpu().numpy(), out2.shape)).T\n",
        "      index = 0\n",
        "      for j in indices2d:\n",
        "        if 78 in top_n[j[0]]: #instead of checking at the end, check here but pronlem: keep the ended one? keep refreshing? use sort not topk?\n",
        "            top_n_.append(list(top_n[j[0]])[:-1]) #remove the <end> so it will come again\n",
        "            # top_n_conf[index]/=softmax_1(out[j[0]][j[1]]) softmax fanle\n",
        "            top_n_conf[index]=1#/=softmax_1(out)[j[0]][j[1]]\n",
        "            continue\n",
        "        top_n_.append(list(top_n[j[0]])+list([j[1]]))\n",
        "        index+=1\n",
        "      count = 0\n",
        "      for i in range(beam):\n",
        "        if 78 in top_n_[i]:\n",
        "          count+=1\n",
        "      if count == beam:\n",
        "        # print(top_n_conf)\n",
        "        return top_n\n",
        "      top_n = top_n_\n",
        "\n",
        "      # print(next)\n",
        "      # conf = conf * softmax(torch.sort(out, descending=True)[0][0].cpu().detach().numpy())[0][0]\n",
        "\n",
        "      # print(text_in)\n",
        "      # count = 0\n",
        "\n",
        "    # print(top_n_conf)\n",
        "    return top_n\n",
        "\n",
        "\n",
        "model = Image2SMILES(encoder, transformer)\n",
        "model.load_state_dict(torch.load(\"./main.pb\", map_location=torch.device(\"cpu\")))\n",
        "gen = SMILESGenerator(model.encoder, model.decoder)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in gen.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = model.to(device)\n",
        "gen = gen.to(device)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  t = np.exp(x)\n",
        "  return t/np.sum(t)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.Softmax(dim=-1)(torch.tensor([[0.1,0.2,3000],[0.2, 0.6, 0.9],[0.9, 0.7, 0.1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meUTmlDN8GJC",
        "outputId": "77c21a13-00e3-4e3e-bb41-26e6646b3f08"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 1.0000],\n",
              "        [0.2219, 0.3311, 0.4469],\n",
              "        [0.4409, 0.3610, 0.1981]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.topk(torch.tensor([[0.1,0.2,0.3],[0.2, 0.6, 0.9],[0.9, 0.7, 0.1]]), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-zo-d-bJljT",
        "outputId": "71f437a3-cbf0-4be4-9254-6dfa86cd408b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([[0.3000],\n",
              "        [0.9000],\n",
              "        [0.9000]]),\n",
              "indices=tensor([[2],\n",
              "        [2],\n",
              "        [0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: transpose torch.broadcast_to(torch.tensor([0.9, 0.1, 0.2]), (10, 3))\n",
        "\n",
        "torch.broadcast_to(torch.tensor([0.9, 0.1, 0.2]), (10, 3)).T * torch.rand((3, 10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLzC4Bp8MXbs",
        "outputId": "4c2624a3-a9d3-48c4-947e-1706776f8f40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7744, 0.3991, 0.0475, 0.6683, 0.1745, 0.1866, 0.3391, 0.2303, 0.4272,\n",
              "         0.0091],\n",
              "        [0.0465, 0.0467, 0.0317, 0.0141, 0.0274, 0.0796, 0.0392, 0.0560, 0.0351,\n",
              "         0.0769],\n",
              "        [0.0470, 0.1398, 0.0448, 0.1876, 0.0311, 0.1456, 0.0763, 0.0431, 0.1516,\n",
              "         0.0944]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ztNaZ26N7wOR"
      },
      "outputs": [],
      "source": [
        "reversed_word_map = {}\n",
        "for i in transformer_.word_map.keys():\n",
        " reversed_word_map[transformer_.word_map[i]] = i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsmOz0DXCH4q"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ox3u4v-69bA",
        "outputId": "68788053-fb85-426f-efb3-cef0133ecd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started\n",
            "CC=O)CC=O)SCCNC=O)C\n",
            "C=CC=CC=C6CN)))NC#N\n",
            "CC=O)CC=O)SCCNC=O)C\n",
            "\n",
            "CCC=CC=CC=C6)O))))OC6\n",
            "CCC=CC=C=C6C6)))))OC6\n",
            "CCC=CC=CC=C6)O))))OC6\n",
            "\n",
            "CCC)C)CCC)C)O)\n",
            "CCCC))CC=CC=C6)))O)O\n",
            "CCC)C)CCC)C)O\n",
            "\n",
            "CCNC=O)C=CC=CC=C6))O\n",
            "CC=C=O)C=CC=C=O)N))O\n",
            "CCNC=O)C=CC=CC=C6))O\n",
            "\n",
            "CNCCO)))C=O)\n",
            "CCCCCC#N+])NC\n",
            "CNCCO)))C=O\n",
            "\n",
            "CC=NNC=S)NC6=O))N\n",
            "CCCCCNC=O)N6=O)C6)O\n",
            "CC=NNC=S)NC6=O))N\n",
            "\n",
            "CS=O)CCCC=O)[O-]))[NH3+]\n",
            "CC=O)CCC=C=O))))C\n",
            "CS=O)CCCC=O)[O-]))[NH3+]\n",
            "\n",
            "CCC)CC=O)C=CC=CC=C6)))))))Br\n",
            "CCCCC=O)O5)Cl))O\n",
            "CCC)CC=O)C=CC=CC=C6)))))))Br\n",
            "\n",
            "CC=O)CCCC4\n",
            "CC=O)=CC=C6S=Cl\n",
            "CC=O)CCCC4\n",
            "\n",
            "C=CC=NC=C6Br))))CF)F)F))\n",
            "CCNCC=CC=C6C=O)C9=NO\n",
            "C=CC=NC=C6Br))))CF)F)F\n",
            "\n",
            "CCNC=NC=CC=CC=C6N9C%10\n",
            "CCCCCCC=C6CC=NO)N\n",
            "CCNC=NC=CC=CC=C6N9C%13\n",
            "\n",
            "CS=O)=O)C=CC=CC=C6))C=O\n",
            "CCCS=O)C=O)O)CC=C=O\n",
            "CS=O)=O)C=CC=CC=C6))C=O\n",
            "\n",
            "C=CC=CC=C6F))F))F)))N\n",
            "C=CC=NON=C6F)F)F\n",
            "C=CC=CC=C6F))F))F)))N\n",
            "\n",
            "C=COC=CN=CC=C6\n",
            "C=C=C=CC=C6O)))C=O\n",
            "C=COC=CN=CC=C6\n",
            "\n",
            "COC=CC=CC=CC=C6N)))))C=C6\n",
            "CC=NCC=C=C=C=O))))O\n",
            "COC=CC=CC=CC=C6N)))))C=C6\n",
            "\n",
            "CC=CC=CN=C6))))Cl\n",
            "CCOCC=O)C=C=C=O\n",
            "CC=CC=CN=C6))))Cl\n",
            "\n",
            "CCC)OC=O)CC)Cl\n",
            "CCCOCC=C=CC6=O6)C=O\n",
            "CCC)OC=O)CC)Cl\n",
            "\n",
            "CCSSCC)C)C\n",
            "CCOCCCCC=C)O)O\n",
            "CCSSCC)C)C\n",
            "\n",
            "CCOCCN6)CO\n",
            "CCCCC=O)Cl\n",
            "CCOCCN6)CO\n",
            "\n",
            "CCCCC=O))))C=O\n",
            "CCCC=CC=C6NC=O))[O\n",
            "CCCCC=O))))C=O\n",
            "\n",
            "C=CC=CC=C6C=O)C=CO6)))))OO))O\n",
            "CC=N=NC=C6NC=C=C6%10\n",
            "C=CC=CC=C6C=O)C=CO6))))))O))O\n",
            "\n",
            "CCCC=O)[O-]))))CCC=O)[O-]\n",
            "CC=CCCCC=C=N=N6)))N\n",
            "CCCC=O)[O-]))))CCC=O)[O-]\n",
            "\n",
            "CC=CC=NC=CC=CC=C%106))))))))N\n",
            "CC=CC=NC=C=NC=C6)O)NCl\n",
            "CC=CC=NC=CC=CC=C%106))))))))N\n",
            "\n",
            "CCCCC=O)Cl))))CC=O)Cl\n",
            "CCCCC=O)CC=O)OC=O)Cl\n",
            "CCCCC=O)Cl))))CC=O)Cl\n",
            "\n",
            "CCCF)F)F))NC=CN=CC=C6\n",
            "CC=CC=NO)=C=C=C=C=C=C6\n",
            "CCCF)F)F))NC=CN=CC=C6\n",
            "\n",
            "CCCCCC6))NN.Cl\n",
            "CCC=CCC6)C6.Cl\n",
            "CCCCCC6))NN.Cl\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-d45036adbb5d>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreversed_word_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<start>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreversed_word_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<start>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreversed_word_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<end>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<start>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<pad>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-4759901a6883>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, text_in_, max_len, beam)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mimage_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_in_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mtop_n_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0;31m# print(top_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "print(\"Started\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "   model.parameters(),\n",
        "   lr=0.0003)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99987)\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def getitem(i):\n",
        " imgs = []\n",
        " ti = []\n",
        " to = []\n",
        " for i in range(i, i+BATCH_SIZE):\n",
        "    index = i\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    try:\n",
        "     img = np.array(Image.open(f\"./rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "     imgs.append(img)\n",
        "     ti.append([77] + Ys[id])\n",
        "     to.append(Ys[id] + [78])\n",
        "    except:\n",
        "     pass\n",
        " return torch.tensor(np.array(imgs), dtype=torch.float32), ti, to\n",
        "\n",
        "Test = True\n",
        "if Test:\n",
        "  model.train(False)\n",
        "  gen.train(False)\n",
        "  running_loss = 0\n",
        "  last_loss = 0\n",
        "  tf_count = 0\n",
        "  bs_count = 0\n",
        "  for i in range(0, 100):\n",
        "    start_index = i * BATCH_SIZE\n",
        "    Xs_img = []\n",
        "    Xs_text = []\n",
        "    y = []\n",
        "\n",
        "    image, text_in, text_out = getitem(start_index)\n",
        "    image = image.to(device)\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    outputs = model(image, text_in, xmask)[0].cpu().detach().numpy()\n",
        "    tf = \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "    bs = \"\".join([reversed_word_map[i] for i in gen(image, [[77]], len(text_in[0]), beam=1)[0]]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "    correct = \"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "    if tf == correct:\n",
        "      tf_count += 1\n",
        "    if bs == correct:\n",
        "      bs_count += 1\n",
        "    print(tf)\n",
        "    print(bs)\n",
        "    print(correct) #print le cai fa xian you <end>\n",
        "    # print(\"Teacher forcing output:\\t\\t\", \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]))\n",
        "    # print(\"Token by Token Output:\\t\\t\", \"\".join([reversed_word_map[i] for i in gen(image, [[77]], len(text_in[0]))[0]]))\n",
        "    # print(\"Correct output:\\t\\t\\t\", \"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]))\n",
        "    print()\n",
        "else:\n",
        "  for epoch in range(30):\n",
        "    image = Xs_img.to(device) #mutli process cannot use cuda so moved here\n",
        "    image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(image, text_in, xmask)\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i%100==99:\n",
        "        wandb.log({\"loss\": running_loss/100, \"acc\":mask_acc(outputs.detach(), text_out), \"lr\": optimizer.param_groups[0]['lr']})\n",
        "        running_loss = 0.\n",
        "    if i%10 == 9:\n",
        "        scheduler.step()\n",
        "\n",
        "    if i == 10:\n",
        "        model.train(False)\n",
        "        for ii in range(0,10):\n",
        "            Xs_img, text_in, text_out = getitem(ii * BATCH_SIZE)\n",
        "\n",
        "            image = Xs_img.to(device) #mutli process cannot use cuda so moved here\n",
        "            image = torch.permute(image, (0, 3, 1, 2))\n",
        "            text_out = pad_pack(text_out)[0].to(device)\n",
        "            padded_x = pad_pack(text_in)\n",
        "            xmask = triangle_mask(padded_x[1]).to(device)\n",
        "            text_in = padded_x[0].to(device)\n",
        "            outputs = model(image, text_in, xmask)\n",
        "            loss = loss_fn(outputs, text_out)\n",
        "            wandb.log({\"val_loss\": loss, \"val_acc\":mask_acc(outputs.detach(), text_out)})\n",
        "        model.train(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1UBI9x9JIm_",
        "outputId": "13911242-76f5-46a5-b66d-4ff8b4aeebf9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfE50lz+y0VbN9BTGmfL5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}