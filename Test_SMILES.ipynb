{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPg6wNOUSYOk1ukqoB0JNAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/Test_SMILES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnyVcrxlz1Rc",
        "outputId": "923ad6bd-d5a2-43b4-a308-6b7227c20ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-11 19:31:59 URL:http://file.weasoft.com/swin_transform_focalloss.pth [2555716718/2555716718] -> \"swin_transform_focalloss.pth\" [1]\n"
          ]
        }
      ],
      "source": [
        "!wget http://file.weasoft.com/main.pb -nv\n",
        "!wget http://file.weasoft.com/80k.csv -nv\n",
        "!wget http://file.weasoft.com/swin_transform_focalloss.pth -nv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://file.weasoft.com/80k.zip"
      ],
      "metadata": {
        "id": "MfwbffN45YLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q 80k.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJzA_gW05u7v",
        "outputId": "538c09b4-c7e1-41bc-9ea3-d80fc0ebe679"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace rendered/140197308_3.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch_xla"
      ],
      "metadata": {
        "id": "HVVr2SEr03VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install deepsmiles yacs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0K1tBQ01d8r",
        "outputId": "fe155383-0497-4ea3-9398-3809b6a2f19e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/suanfaxiaohuo/SwinOCSR.git"
      ],
      "metadata": {
        "id": "5XIHVPNJ1lSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import pylab\n",
        "import pandas\n",
        "import sys\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "rovlreP90bP5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    HOME_DIR = \"/content\"\n",
        "else:\n",
        "    HOME_DIR = \"/arc/burst/st-dushan20-1/\""
      ],
      "metadata": {
        "id": "KA3vnh8U1Nkd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# From the SwinOCSR\n",
        "import sys\n",
        "sys.path.append(\"./SwinOCSR/model/Swin-transformer-focalloss\")\n",
        "sys.path.append(\"./SwinOCSR/model/\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import deepsmiles\n",
        "from typing import Any, cast, Callable, List, Tuple, Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from pre_transformer import Transformer"
      ],
      "metadata": {
        "id": "au-7ZSY81Slk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  device = xm.xla_device()\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda:0')\n",
        "  else:\n",
        "      device = torch.device('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL5Wdj831EqV",
        "outputId": "3bf34d80-d12f-45f7-8546-635dd690e9fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
            "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLossModelInference:\n",
        "    \"\"\"\n",
        "    Inference Class\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Load dictionary that maps tokens to integers\n",
        "        word_map_path = './SwinOCSR/Data/500wan/500wan_shuffle_DeepSMILES_word_map'\n",
        "        self.word_map = torch.load(word_map_path)\n",
        "        self.inv_word_map = {v: k for k, v in self.word_map.items()}\n",
        "\n",
        "        # Define device, load models and weights\n",
        "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # self.args, config = self.get_inference_config()\n",
        "        # self.encoder = build_model(config, tag=False)\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.load_checkpoint(\"./swin_transform_focalloss.pth\")\n",
        "        self.decoder = self.decoder.to(self.dev).eval()\n",
        "        # self.encoder = self.encoder.to(self.dev).eval()\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load checkpoint and update encoder and decoder accordingly\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path (str): path of checkpoint file\n",
        "        \"\"\"\n",
        "        print(f\"=====> {checkpoint_path} <=====\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        # encoder_msg = self.encoder.load_state_dict(checkpoint['encoder'],\n",
        "        #                                            strict=False)\n",
        "        decoder_msg = self.decoder.load_state_dict(checkpoint['decoder'],\n",
        "                                                   strict=False)\n",
        "        # print(f\"Encoder: {encoder_msg}\")\n",
        "        print(f\"Decoder: {decoder_msg}\")\n",
        "        del checkpoint\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def build_decoder(self):\n",
        "        \"\"\"\n",
        "        This method builds the Transformer decoder and returns it\n",
        "        \"\"\"\n",
        "        self.decoder_dim = 256  # dimension of decoder RNN\n",
        "        self.ff_dim = 2048\n",
        "        self.num_head = 8\n",
        "        self.dropout = 0.1\n",
        "        self.encoder_num_layer = 6\n",
        "        self.decoder_num_layer = 6\n",
        "        self.max_len = 277\n",
        "        self.decoder_lr = 5e-4\n",
        "        self.best_acc = 0.\n",
        "        return Transformer(dim=self.decoder_dim,\n",
        "                           ff_dim=self.ff_dim,\n",
        "                           num_head=self.num_head,\n",
        "                           encoder_num_layer=self.encoder_num_layer,\n",
        "                           decoder_num_layer=self.decoder_num_layer,\n",
        "                           vocab_size=len(self.word_map),\n",
        "                           max_len=self.max_len,\n",
        "                           drop_rate=self.dropout,\n",
        "                           tag=False)\n",
        "transformer_ = FocalLossModelInference()\n",
        "transformer = transformer_.build_decoder().decoder\n",
        "\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "\n",
        "\n",
        "def str_to_vector(s: str)->list:\n",
        "  return [transformer_.word_map[i] for i in converter.encode(s)]\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "csv = pd.read_csv(\"./80k.csv\")\n",
        "cids = csv[\"cid\"]\n",
        "csv.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWiFh9bA1wUV",
        "outputId": "2c7f555c-0411-49fc-a60c-2352d95a928e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> ./swin_transform_focalloss.pth <=====\n",
            "Decoder: <All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['cid', 'cmpdname', 'cmpdsynonym', 'mw', 'mf', 'polararea', 'complexity',\n",
              "       'xlogp', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'inchi',\n",
              "       'isosmiles', 'canonicalsmiles', 'inchikey', 'iupacname', 'exactmass',\n",
              "       'monoisotopicmass', 'charge', 'covalentunitcnt', 'isotopeatomcnt',\n",
              "       'totalatomstereocnt', 'definedatomstereocnt', 'undefinedatomstereocnt',\n",
              "       'totalbondstereocnt', 'definedbondstereocnt', 'undefinedbondstereocnt',\n",
              "       'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'neighbortype', 'meshheadings',\n",
              "       'annothits', 'annothitcnt', 'aids', 'cidcdate', 'sidsrcname', 'depcatg',\n",
              "       'annotation'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ys = {}\n",
        "invalid_cids = []\n",
        "for i in cids.values:\n",
        "    tmp = str_to_vector(csv[csv[\"cid\"] == i][\"canonicalsmiles\"].values[0])\n",
        "    if not tmp == None:\n",
        "      Ys[i] = tmp\n",
        "    else:\n",
        "      invalid_cids.append(i)\n",
        "\n",
        "print(\"OOHH\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L9d_nQJ2V_m",
        "outputId": "2320c5d6-0c7c-49d7-80f2-350c73c43d78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOHH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_in = Image.open(\"./rendered/6912034_0.jpg\")\n",
        "example_out = csv[csv[\"cid\"]==6912034][\"canonicalsmiles\"].values[0]\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "example_out = str_to_vector(example_out)\n",
        "\n",
        "\n",
        "torch.permute(torch.tensor(np.expand_dims(example_in, 0).astype(\"float32\")), (0,3,1,2)).size()\n",
        "\n",
        "\n",
        "t = torch.randn(1,64,400,400)\n",
        "torch.flatten(t)\n",
        "torch.flatten(t, start_dim=2,end_dim=3).shape\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "import numpy as np\n",
        "def positional_encoding_1d(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return pos_encoding\n",
        "\n",
        "\n",
        "def positional_encoding_2d(row, col, d_model):\n",
        "    assert d_model % 2 == 0\n",
        "    row_pos = np.repeat(np.arange(row), col)[:, np.newaxis]\n",
        "    col_pos = np.repeat(np.expand_dims(np.arange(col), 0), row, axis=0).reshape(-1, 1)\n",
        "\n",
        "    angle_rads_row = get_angles(\n",
        "        row_pos, np.arange(d_model // 2)[np.newaxis, :], d_model // 2\n",
        "    )\n",
        "    angle_rads_col = get_angles(\n",
        "        col_pos, np.arange(d_model // 2)[np.newaxis, :], d_model // 2\n",
        "    )\n",
        "\n",
        "    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n",
        "    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n",
        "    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n",
        "    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n",
        "    pos_encoding = np.concatenate([angle_rads_row, angle_rads_col], axis=1)[\n",
        "        np.newaxis, ...\n",
        "    ]\n",
        "    return pos_encoding\n",
        "\n",
        "eff = torchvision.models.efficientnet_v2_l(weights='DEFAULT')\n",
        "mynet = eff.features\n",
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eff = mynet.to(device)\n",
        "    self.projection = torch.nn.Linear(1280,256).to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    features = torch.flatten(features, start_dim=2, end_dim=3)\n",
        "    features = torch.permute(features, (0, 2, 1))\n",
        "    return self.projection(features)\n",
        "\n",
        "\n",
        "encoder = ImageEncoder()\n",
        "\n",
        "for p in transformer.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "\n",
        "# In[45]:\n",
        "# from another paper\n",
        "def pad_pack(sequences):\n",
        "    maxlen = max(map(len, sequences))\n",
        "    batch = torch.LongTensor(len(sequences),maxlen).fill_(0)\n",
        "    for i,x in enumerate(sequences):\n",
        "        batch[i,:len(x)] = torch.LongTensor(x)\n",
        "    return batch, maxlen\n",
        "\n",
        "# https://github.com/suanfaxiaohuo/SwinOCSR/blob/main/model/Swin-transformer-focalloss/pre_transformer.py#L95\n",
        "def triangle_mask(size):\n",
        "    mask = 1- np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
        "    mask = torch.autograd.Variable(torch.from_numpy(mask))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in, xmask):\n",
        "    #padded_text, maxlen = pad_pack(text_in)\n",
        "    #padded_text = padded_text.to(device)\n",
        "    # image = torch.permute(torch.tensor(np.array(image)), (0,3,1,2))\n",
        "    image_feature = self.encoder(image)\n",
        "    out = self.decoder(text_in, image_feature, x_mask=xmask)#triangle_mask(maxlen).to(device))\n",
        "    return out\n",
        "\n",
        "# In[46]:\n",
        "\n",
        "def softmax(x):\n",
        "        t = np.exp(x)\n",
        "        return t/np.sum(t)\n",
        "class SMILESGenerator(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward (self, image, text_in, max_len):\n",
        "    image_feature = self.encoder(image)\n",
        "    conf = 1\n",
        "    for i in range(max_len):\n",
        "      padded_text = pad_pack(text_in)[0]\n",
        "      padded_text = padded_text.to(device)\n",
        "      out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(len(text_in)).to(device))\n",
        "      # out = self.generator(out)\n",
        "      next = torch.sort(out, descending=True)[1][0,0].cpu().detach().numpy()[0] #forgot descending\n",
        "      conf = conf * softmax(torch.sort(out, descending=True)[0][0].cpu().detach().numpy())[0][0]\n",
        "      #if next == 3:\n",
        "      #    text_in.append(3)\n",
        "      #    break\n",
        "      text_in[0] += [next]\n",
        "      # print(text_in)\n",
        "    return (text_in[0]), text_in[0], conf\n",
        "\n",
        "model = Image2SMILES(encoder, transformer)\n",
        "gen = SMILESGenerator(encoder, transformer)\n",
        "\n",
        "model = model.to(device)\n",
        "gen = gen.to(device)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  t = np.exp(x)\n",
        "  return t/np.sum(t)\n"
      ],
      "metadata": {
        "id": "DwuU7Rrp53Sw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal_loss_torch wandb"
      ],
      "metadata": {
        "id": "3qbQiwEx6iXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from focal_loss.focal_loss import FocalLoss\n",
        "m = torch.nn.Softmax(dim=-1)\n",
        "lf = FocalLoss(gamma=2, ignore_index=0)#torch.nn.CrossEntropyLoss(label_smoothing=0.1, reduction=\"none\")\n",
        "def loss_fn(pred, truth):\n",
        "  pred = m(pred)\n",
        "  l = lf(pred, truth)\n",
        "  return l\n",
        "\n",
        "\n",
        "def mask_acc(pred, truth):\n",
        "    pred = torch.argmax(pred, -1)\n",
        "    mask = truth != 0\n",
        "    match_case = truth == pred\n",
        "    return torch.sum(mask*match_case)/torch.sum(mask)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "files = os.listdir(f\"./rendered/\")\n",
        "files = [i for i in files if len(i)<=40]\n",
        "import multiprocessing, threading\n",
        "#https://stackoverflow.com/questions/48822463/how-to-use-pytorch-multiprocessing\n",
        "from torch.multiprocessing import Process, Queue, Pool\n",
        "import time\n",
        "buffer = Queue(maxsize=10) #need maxsize=10, otherwise put will also block\n",
        "start_index = 0\n",
        "import cv2\n",
        "def process_single(arg):\n",
        "    # print(arg)\n",
        "    _, start_index = arg\n",
        "    index = start_index + _\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    img = np.array(Image.open(f\"{HOME_DIR}/rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "    return img, [77] + Ys[id], Ys[id] + [78]\n",
        "\n",
        "#pool = Pool()\n",
        "\n",
        "\n",
        "#read all images\n",
        "images_warehouse = []\n",
        "index = 0\n",
        "import time\n",
        "t0 = time.time()\n",
        "import os, psutil\n",
        "process = psutil.Process()\n"
      ],
      "metadata": {
        "id": "Vt1J6ydd6cht"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reversed_word_map = {}\n",
        "for i in transformer_.word_map.keys():\n",
        " reversed_word_map[transformer_.word_map[i]] = i\n"
      ],
      "metadata": {
        "id": "ztNaZ26N7wOR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "print(\"Started\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "   lr=0.0003)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99987)\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def getitem(i):\n",
        " imgs = []\n",
        " ti = []\n",
        " to = []\n",
        " for i in range(i, i+BATCH_SIZE):\n",
        "    index = i\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    try:\n",
        "     img = np.array(Image.open(f\"{HOME_DIR}/rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "     imgs.append(img)\n",
        "     ti.append([77] + Ys[id])\n",
        "     to.append(Ys[id] + [78])\n",
        "    except:\n",
        "     pass\n",
        " return torch.tensor(np.array(imgs), dtype=torch.float32), ti, to\n",
        "\n",
        "Test = True\n",
        "if Test:\n",
        "  model.train(False)\n",
        "  running_loss = 0\n",
        "  last_loss = 0\n",
        "  for i in range(10, len(files)//BATCH_SIZE-1):\n",
        "    start_index = i * BATCH_SIZE\n",
        "    Xs_img = []\n",
        "    Xs_text = []\n",
        "    y = []\n",
        "\n",
        "    image, text_in, text_out = getitem(start_index)\n",
        "    image = image.to(device)\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    outputs = model(image, text_in, xmask)[0].cpu().detach().numpy()\n",
        "    print(\"Teacher forcing Predicted output:\\t\\t\", \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]))\n",
        "    print(\"Token by Token Output: \", \"\".join([reversed_word_map[np.argmax(i)] for i in gen(image, [[77]])]))\n",
        "    print(\"Correct output:\\t\\t\", \",\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]))\n",
        "    print()\n",
        "else:\n",
        "  for epoch in range(30):\n",
        "    image = Xs_img.to(device) #mutli process cannot use cuda so moved here\n",
        "    image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(image, text_in, xmask)\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i%100==99:\n",
        "        wandb.log({\"loss\": running_loss/100, \"acc\":mask_acc(outputs.detach(), text_out), \"lr\": optimizer.param_groups[0]['lr']})\n",
        "        running_loss = 0.\n",
        "    if i%10 == 9:\n",
        "        scheduler.step()\n",
        "\n",
        "    if i == 10:\n",
        "        torch.save(model.state_dict(), f\"/scratch/st-dushan20-1/final_nomlp_{epoch}_{i}.mod\")\n",
        "        #finally, let's do some val, data from 0-10 are val data\n",
        "        model.train(False)\n",
        "        for ii in range(0,10):\n",
        "            Xs_img, text_in, text_out = getitem(ii * BATCH_SIZE)\n",
        "\n",
        "            image = Xs_img.to(device) #mutli process cannot use cuda so moved here\n",
        "            image = torch.permute(image, (0, 3, 1, 2))\n",
        "            text_out = pad_pack(text_out)[0].to(device)\n",
        "            padded_x = pad_pack(text_in)\n",
        "            xmask = triangle_mask(padded_x[1]).to(device)\n",
        "            text_in = padded_x[0].to(device)\n",
        "            outputs = model(image, text_in, xmask)\n",
        "            loss = loss_fn(outputs, text_out)\n",
        "            wandb.log({\"val_loss\": loss, \"val_acc\":mask_acc(outputs.detach(), text_out)})\n",
        "        model.train(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ox3u4v-69bA",
        "outputId": "dbc4d8eb-4275-4654-ff0c-c4c3ae228a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started\n"
          ]
        }
      ]
    }
  ]
}