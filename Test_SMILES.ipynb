{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/Test_SMILES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AnyVcrxlz1Rc"
      },
      "outputs": [],
      "source": [
        "# !wget http://file.weasoft.com/80k.csv -nv\n",
        "# !wget http://file.weasoft.com/swin_transform_focalloss.pth -nv\n",
        "# !wget http://file.weasoft.com/80k.zip\n",
        "# !unzip -q 80k.zip\n",
        "# !pip3 install torch_xla\n",
        "# !pip3 install deepsmiles yacs tqdm\n",
        "# !git clone https://github.com/suanfaxiaohuo/SwinOCSR.git\n",
        "# !pip install focal_loss_torch wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://file.weasoft.com/main.pb -nv"
      ],
      "metadata": {
        "id": "ddVV_PEkMap-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rovlreP90bP5"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import pylab\n",
        "import pandas\n",
        "import sys\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KA3vnh8U1Nkd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    HOME_DIR = \"/content\"\n",
        "else:\n",
        "    HOME_DIR = \"/arc/burst/st-dushan20-1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "au-7ZSY81Slk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# From the SwinOCSR\n",
        "import sys\n",
        "sys.path.append(\"/content/SwinOCSR/model/Swin-transformer-focalloss\")\n",
        "sys.path.append(\"./SwinOCSR/model/\")\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import deepsmiles\n",
        "from typing import Any, cast, Callable, List, Tuple, Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from pre_transformer import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BL5Wdj831EqV"
      },
      "outputs": [],
      "source": [
        "# os.environ['PJRT_DEVICE'] = 'TPU' #That is why\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.runtime as xr\n",
        "# device = xm.xla_device()\n",
        "try:\n",
        "  5/0\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xmc\n",
        "  device = xm.xla_device()\n",
        "except:\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda:0')\n",
        "  else:\n",
        "      device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kt24shs48VFU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWIER9968W90"
      },
      "source": [
        "# Swin-OCSR Def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWiFh9bA1wUV",
        "outputId": "f4407d2a-b6e0-4ff7-e2ee-b7a4dec20eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> ./swin_transform_focalloss.pth <=====\n",
            "Decoder: <All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['cid', 'cmpdname', 'cmpdsynonym', 'mw', 'mf', 'polararea', 'complexity',\n",
              "       'xlogp', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'inchi',\n",
              "       'isosmiles', 'canonicalsmiles', 'inchikey', 'iupacname', 'exactmass',\n",
              "       'monoisotopicmass', 'charge', 'covalentunitcnt', 'isotopeatomcnt',\n",
              "       'totalatomstereocnt', 'definedatomstereocnt', 'undefinedatomstereocnt',\n",
              "       'totalbondstereocnt', 'definedbondstereocnt', 'undefinedbondstereocnt',\n",
              "       'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'neighbortype', 'meshheadings',\n",
              "       'annothits', 'annothitcnt', 'aids', 'cidcdate', 'sidsrcname', 'depcatg',\n",
              "       'annotation'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "class FocalLossModelInference:\n",
        "    \"\"\"\n",
        "    Inference Class\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Load dictionary that maps tokens to integers\n",
        "        word_map_path = './SwinOCSR/Data/500wan/500wan_shuffle_DeepSMILES_word_map'\n",
        "        self.word_map = torch.load(word_map_path)\n",
        "        self.inv_word_map = {v: k for k, v in self.word_map.items()}\n",
        "\n",
        "        # Define device, load models and weights\n",
        "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # self.args, config = self.get_inference_config()\n",
        "        # self.encoder = build_model(config, tag=False)\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.load_checkpoint(\"./swin_transform_focalloss.pth\")\n",
        "        self.decoder = self.decoder.to(self.dev).eval()\n",
        "        # self.encoder = self.encoder.to(self.dev).eval()\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load checkpoint and update encoder and decoder accordingly\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path (str): path of checkpoint file\n",
        "        \"\"\"\n",
        "        print(f\"=====> {checkpoint_path} <=====\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        # encoder_msg = self.encoder.load_state_dict(checkpoint['encoder'],\n",
        "        #                                            strict=False)\n",
        "        decoder_msg = self.decoder.load_state_dict(checkpoint['decoder'],\n",
        "                                                   strict=False)\n",
        "        # print(f\"Encoder: {encoder_msg}\")\n",
        "        print(f\"Decoder: {decoder_msg}\")\n",
        "        del checkpoint\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def build_decoder(self):\n",
        "        \"\"\"\n",
        "        This method builds the Transformer decoder and returns it\n",
        "        \"\"\"\n",
        "        self.decoder_dim = 256  # dimension of decoder RNN\n",
        "        self.ff_dim = 2048\n",
        "        self.num_head = 8\n",
        "        self.dropout = 0.1\n",
        "        self.encoder_num_layer = 6\n",
        "        self.decoder_num_layer = 6\n",
        "        self.max_len = 277\n",
        "        self.decoder_lr = 5e-4\n",
        "        self.best_acc = 0.\n",
        "        return Transformer(dim=self.decoder_dim,\n",
        "                           ff_dim=self.ff_dim,\n",
        "                           num_head=self.num_head,\n",
        "                           encoder_num_layer=self.encoder_num_layer,\n",
        "                           decoder_num_layer=self.decoder_num_layer,\n",
        "                           vocab_size=len(self.word_map),\n",
        "                           max_len=self.max_len,\n",
        "                           drop_rate=self.dropout,\n",
        "                           tag=False)\n",
        "transformer_ = FocalLossModelInference()\n",
        "transformer = transformer_.build_decoder().decoder\n",
        "\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "\n",
        "\n",
        "def str_to_vector(s: str)->list:\n",
        "  return [transformer_.word_map[i] for i in converter.encode(s)]\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "csv = pd.read_csv(\"./80k.csv\")\n",
        "cids = csv[\"cid\"]\n",
        "csv.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88z_svB78Z5x"
      },
      "source": [
        "# pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9L9d_nQJ2V_m"
      },
      "outputs": [],
      "source": [
        "# prompt: Ys = {} invalid_cids = [] for i in cids.values: tmp = str_to_vector(csv[csv[\"cid\"] == i][\"canonicalsmiles\"].values[0]) if not tmp == None: Ys[i] = tmp else: invalid_cids.append(i)  try to copy Ys.pkl from users drive if it exsits skip this and just load from Ys.pkl other wise run this and save Ys and invalid cids in Ys.pkl. Then copy this to users google drive\n",
        "\n",
        "import pickle\n",
        "try:\n",
        "  with open(HOME_DIR + \"/Ys.pkl\", \"rb\") as f:\n",
        "    Ys, invalid_cids = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "  Ys = {}\n",
        "  invalid_cids = []\n",
        "  for i in cids.values:\n",
        "    tmp = str_to_vector(csv[csv[\"cid\"] == i][\"canonicalsmiles\"].values[0])\n",
        "    if not tmp == None:\n",
        "      Ys[i] = tmp\n",
        "    else:\n",
        "      invalid_cids.append(i)\n",
        "  with open(HOME_DIR + \"/Ys.pkl\", \"wb\") as f:\n",
        "    pickle.dump([Ys, invalid_cids], f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vt1J6ydd6cht"
      },
      "outputs": [],
      "source": [
        "from focal_loss.focal_loss import FocalLoss\n",
        "m = torch.nn.Softmax(dim=-1)\n",
        "lf = FocalLoss(gamma=2, ignore_index=0)#torch.nn.CrossEntropyLoss(label_smoothing=0.1, reduction=\"none\")\n",
        "def loss_fn(pred, truth):\n",
        "  pred = m(pred)\n",
        "  l = lf(pred, truth)\n",
        "  return l\n",
        "\n",
        "def mask_acc(pred, truth):\n",
        "    pred = torch.argmax(pred, -1)\n",
        "    mask = truth != 0\n",
        "    match_case = truth == pred\n",
        "    return torch.sum(mask*match_case)/torch.sum(mask)\n",
        "\n",
        "import pickle\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "files = os.listdir(f\"./rendered/\")\n",
        "files = [i for i in files if len(i)<=40]\n",
        "import multiprocessing, threading\n",
        "#https://stackoverflow.com/questions/48822463/how-to-use-pytorch-multiprocessing\n",
        "from torch.multiprocessing import Process, Queue, Pool\n",
        "import time\n",
        "buffer = Queue(maxsize=10) #need maxsize=10, otherwise put will also block\n",
        "start_index = 0\n",
        "import cv2\n",
        "def process_single(arg):\n",
        "    # print(arg)\n",
        "    _, start_index = arg\n",
        "    index = start_index + _\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    img = np.array(Image.open(f\"{HOME_DIR}/rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "    return img, [77] + Ys[id], Ys[id] + [78]\n",
        "\n",
        "images_warehouse = []\n",
        "index = 0\n",
        "import time\n",
        "t0 = time.time()\n",
        "import os, psutil\n",
        "process = psutil.Process()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LPAoMbiFdb5"
      },
      "source": [
        "# reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y5ta9TW7ou_5"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DwuU7Rrp53Sw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# https://stackoverflow.com/questions/70508960/how-to-free-gpu-memory-in-pytorch\n",
        "\n",
        "try:\n",
        "  model.cpu()\n",
        "  gen.cpu()\n",
        "  del model, gen\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "except:\n",
        "  pass\n",
        "example_in = Image.open(\"./rendered/6912034_0.jpg\")\n",
        "example_out = csv[csv[\"cid\"]==6912034][\"canonicalsmiles\"].values[0]\n",
        "\n",
        "# In[15]:\n",
        "example_out = str_to_vector(example_out)\n",
        "import numpy as np\n",
        "\n",
        "eff = torchvision.models.efficientnet_v2_l(weights='DEFAULT')\n",
        "mynet = eff.features\n",
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eff = mynet.to(device)\n",
        "    self.projection = torch.nn.Linear(1280,256).to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    features = torch.flatten(features, start_dim=2, end_dim=3)\n",
        "    features = torch.permute(features, (0, 2, 1))\n",
        "    return self.projection(features)\n",
        "\n",
        "\n",
        "\n",
        "encoder = ImageEncoder()\n",
        "\n",
        "for p in transformer.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "\n",
        "# In[45]:\n",
        "# from another paper\n",
        "def pad_pack(sequences):\n",
        "    maxlen = max(map(len, sequences))\n",
        "    batch = torch.LongTensor(len(sequences),maxlen).fill_(0)\n",
        "    for i,x in enumerate(sequences):\n",
        "        batch[i,:len(x)] = torch.LongTensor(x)\n",
        "    return batch, maxlen\n",
        "\n",
        "# https://github.com/suanfaxiaohuo/SwinOCSR/blob/main/model/Swin-transformer-focalloss/pre_transformer.py#L95\n",
        "def triangle_mask(size):\n",
        "    mask = 1- np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
        "    mask = torch.autograd.Variable(torch.from_numpy(mask))\n",
        "    return mask\n",
        "\n",
        "\n",
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in, xmask):\n",
        "    image_feature = self.encoder(image)\n",
        "    out = self.decoder(text_in, image_feature, x_mask=xmask)\n",
        "    return out\n",
        "\n",
        "# In[46]:\n",
        "\n",
        "def softmax(x):\n",
        "        t = np.exp(x)\n",
        "        return t/np.sum(t)\n",
        "\n",
        "softmax_1 = torch.nn.Softmax(dim=-1)\n",
        "torch_softmax = torch.nn.Softmax()\n",
        "class SMILESGenerator(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in_, max_len, beam): #just changed beam=1 and it runs so beam cannot be random number???  yeah it has to been factor of 676, which is 2 2 13 13 but original paper did not use beam search\n",
        "    image_feature = self.encoder(image)\n",
        "    top_n = list([list(i) for i in torch.tensor(text_in_).repeat(beam,1).detach().cpu().numpy()])\n",
        "    # print(top_n)\n",
        "    image_feature = image_feature.repeat_interleave(beam, dim=0)#.repeat_interleave(beam)\n",
        "    for i in range(max_len):\n",
        "      if beam == 1:\n",
        "        padded_text, l = pad_pack(top_n)\n",
        "        padded_text = padded_text.to(device)\n",
        "        out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(l).to(device))\n",
        "        out = out[0,-1,:]\n",
        "        next = torch.argmax(out)\n",
        "        top_n[0] += [next.detach().cpu().item()]\n",
        "        if next == 78:\n",
        "          return top_n\n",
        "        continue\n",
        "      padded_text, l = pad_pack(top_n)\n",
        "      padded_text = padded_text.to(device)\n",
        "      out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(l).to(device))\n",
        "      out = out[:,-1,:]\n",
        "      next = torch.topk(torch.flatten(out), beam)\n",
        "      indices2d = np.array(np.unravel_index(next.indices.cpu().numpy(), out.shape)).T\n",
        "      new_top_n = []\n",
        "      count = 0\n",
        "      for j in indices2d:\n",
        "        beam_number = j[0]\n",
        "        char = j[1]\n",
        "        if char == 78:\n",
        "          count += 1\n",
        "          new_top_n.append(top_n[beam_number])\n",
        "        else:\n",
        "          new_top_n.append(top_n[beam_number] + [char])\n",
        "      if count == beam:\n",
        "        return new_top_n\n",
        "      top_n = new_top_n\n",
        "    return top_n\n",
        "\n",
        "\n",
        "model = Image2SMILES(encoder, transformer)\n",
        "model.load_state_dict(torch.load(\"./main.pb\", map_location=device))\n",
        "gen = SMILESGenerator(model.encoder, model.decoder)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in gen.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.decoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model = model.to(device)\n",
        "gen = gen.to(device)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "  t = np.exp(x)\n",
        "  return t/np.sum(t)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[77]]).repeat_interleave(2, dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45amkCSfaHSs",
        "outputId": "9ff5abe3-d3c7-405f-a4b3-98186fa70a77"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[77],\n",
              "        [77]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -"
      ],
      "metadata": {
        "id": "PkHPwO_ZN0hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ztNaZ26N7wOR"
      },
      "outputs": [],
      "source": [
        "reversed_word_map = {}\n",
        "for i in transformer_.word_map.keys():\n",
        " reversed_word_map[transformer_.word_map[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ox3u4v-69bA",
        "outputId": "7c7e6e4c-6518-4ee3-c438-e8d6440c795a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "print(\"Started\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "   model.parameters(),\n",
        "   lr=0.0003)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99987)\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def getitem(i):\n",
        " imgs = []\n",
        " ti = []\n",
        " to = []\n",
        " for i in range(i, i+BATCH_SIZE):\n",
        "    index = i\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      return\n",
        "    try:\n",
        "     img = np.array(Image.open(f\"./rendered/{files[index]}\")) #cannot read image? quota reached?\n",
        "     imgs.append(img)\n",
        "     ti.append([77] + Ys[id])\n",
        "     to.append(Ys[id] + [78])\n",
        "    except:\n",
        "     pass\n",
        " return torch.tensor(np.array(imgs), dtype=torch.float32), ti, to\n",
        "\n",
        "Test = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErnShHl8--AK",
        "outputId": "06dd4020-d135-426d-ec5e-d3b5453e6b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwguo6358\u001b[0m (\u001b[33m3dsmile\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nF6FRZ81Wxo"
      },
      "outputs": [],
      "source": [
        "if Test:\n",
        "  model.train(False)\n",
        "  gen.train(False)\n",
        "  running_loss = 0\n",
        "  last_loss = 0\n",
        "  tf_count = 0\n",
        "  bs_count = 0\n",
        "  ans = {}\n",
        "  for beam_count in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
        "    ans[beam_count] = 0\n",
        "    for i in range(0, 1000):\n",
        "      start_index = i * BATCH_SIZE\n",
        "      Xs_img = []\n",
        "      Xs_text = []\n",
        "      y = []\n",
        "\n",
        "      image, text_in, text_out = getitem(start_index)\n",
        "      image = image.to(device)\n",
        "      text_out = pad_pack(text_out)[0].to(device)\n",
        "      padded_x = pad_pack(text_in)\n",
        "      xmask = triangle_mask(padded_x[1]).to(device)\n",
        "      image = torch.permute(image, (0, 3, 1, 2))\n",
        "      text_in = padded_x[0].to(device)\n",
        "      outputs = model(image, text_in, xmask)[0].cpu().detach().numpy()\n",
        "      # tf = \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "      bs = \"\".join([reversed_word_map[i] for i in gen(image[:1,:,:,:], [[77]], len(text_in[0]), beam=beam_count)[0]]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "      #  try textin[:,:-1] then -3 then 1 and it work,:0s and now i got why the beam used to need to be factor that is because image side is that i need repeat image side\n",
        "      correct = \"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "      # if tf == correct:\n",
        "      #   tf_count += 1\n",
        "      if bs == correct:\n",
        "        bs_count += 1\n",
        "        ans[beam_count] += 1\n",
        "\n",
        "      # print(tf)\n",
        "      # print(bs)\n",
        "      # print(correct) #print le cai fa xian you <end>\n",
        "      # print(\"Teacher forcing output:\\t\\t\", \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]))\n",
        "      # print(\"Token by Token Output:\\t\\t\", \"\".join([reversed_word_map[i] for i in gen(image, [[77]], len(text_in[0]))[0]]))\n",
        "      # print(\"Correct output:\\t\\t\\t\", \"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# haihao haibaoluzheruntim just ntoice that duzie i say if there is batch norm in image encoder wait what is the batch size there then relized there are 4 images\n",
        "print(\"\".join([reversed_word_map[i] for i in gen(image[:1,:,:,:], [[77]], len(text_in[0]), beam=1)[0]]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\"))\n",
        "print(\"\".join([reversed_word_map[i] for i in gen(image, [[77]]*4, len(text_in[0]), beam=1)[0]]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\"))\n",
        "print(\"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGKVnHeSQTZJ",
        "outputId": "01216597-c118-435d-9e42-93fc9d9dc2a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[77]]\n",
            "CCNC=S)N\n",
            "[[77], [77], [77], [77]]\n",
            "CCNC=S)N\n",
            "CCNC=S)N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image[:1,:,:,:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwHQwQuJXo1G",
        "outputId": "807ceb10-63c1-4269-bb92-89d3687b7a16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 400, 400])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAYrPV-PmhS0"
      },
      "outputs": [],
      "source": [
        "for epoch in range(30):\n",
        "  model.train(True)\n",
        "\n",
        "  for i in range(10, len(files)//BATCH_SIZE):\n",
        "    image, text_in, text_out = getitem(start_index)\n",
        "\n",
        "    image = image.to(device) #mutli process cannot use cuda so moved here\n",
        "    image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(image, text_in, xmask)\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i%100==99:\n",
        "        wandb.log({\"loss\": running_loss/100, \"acc\":mask_acc(outputs.detach(), text_out), \"lr\": optimizer.param_groups[0]['lr']})\n",
        "        running_loss = 0.\n",
        "    if i%10 == 9:\n",
        "        scheduler.step()\n",
        "\n",
        "    if i == 10:\n",
        "        model.train(False)\n",
        "        loss_ = 0\n",
        "        acc_ = 0\n",
        "        for ii in range(0,10):\n",
        "            tf = \"\".join([reversed_word_map[np.argmax(i)] for i in outputs]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "            bs = \"\".join([reversed_word_map[i] for i in gen(image, text_in[:,:1], len(text_in[0]), beam=1)[0]]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "            correct = \"\".join([reversed_word_map[i] for i in text_out[0].detach().cpu().numpy()]).replace(\"<end>\",\"\").replace(\"<start>\",\"\").replace(\"<pad>\",\"\")\n",
        "            print(f\"tf:\\t{tf}\\n bs:\\t{bs}\\n correct:\\t{correct}\\n\")\n",
        "\n",
        "            Xs_img, text_in, text_out = getitem(ii * BATCH_SIZE)\n",
        "            image = Xs_img.to(device)\n",
        "            image = torch.permute(image, (0, 3, 1, 2))\n",
        "            text_out = pad_pack(text_out)[0].to(device)\n",
        "            padded_x = pad_pack(text_in)\n",
        "            xmask = triangle_mask(padded_x[1]).to(device)\n",
        "            text_in = padded_x[0].to(device)\n",
        "            text_in = torch.permute(text_in, (1,0))\n",
        "            outputs = model(image, text_in, xmask)\n",
        "            loss = loss_fn(outputs, text_out)\n",
        "            loss_ += loss\n",
        "            acc_ += mask_acc(outputs.detach(), text_out)\n",
        "        wandb.log({\"val_loss\": loss/10, \"val_acc\":acc_/10})\n",
        "        model.train(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pylab\n",
        "keys = ans.keys()\n",
        "values = ans.values()\n",
        "pylab.plot(keys, values)\n",
        "pylab.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "bjq2vQOZewiM",
        "outputId": "7d82df22-1c15-41c2-d891-98c784377b72"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43TrShMbe5Qg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xWIER9968W90"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMhhiREn/hiqAVmBPe5skYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}