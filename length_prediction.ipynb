{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1dcI8O0OQoXZbYfvGbf7rY0IjY-0FIs1W",
      "authorship_tag": "ABX9TyNtW3d1HW0t397klQgGeoNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/length_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepsmiles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUFeQFw_I4Nk",
        "outputId": "f8e95c59-56a9-4bac-b447-c01c7eb27513"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepsmiles in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiFRxOs1-Rky",
        "outputId": "a1740965-b2b7-4797-d963-02e963ecc817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-12 04:21:38--  http://file.weasoft.com/images.zip\n",
            "Resolving file.weasoft.com (file.weasoft.com)... 149.28.13.194\n",
            "Connecting to file.weasoft.com (file.weasoft.com)|149.28.13.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 265877516 (254M) [application/zip]\n",
            "Saving to: ‘images.zip.3’\n",
            "\n",
            "images.zip.3          0%[                    ]       0  --.-KB/s               ^C\n",
            "--2024-02-12 04:21:39--  http://file.weasoft.com/summary.csv\n",
            "Resolving file.weasoft.com (file.weasoft.com)... 149.28.13.194\n",
            "Connecting to file.weasoft.com (file.weasoft.com)|149.28.13.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 326352 (319K) [text/csv]\n",
            "Saving to: ‘summary.csv.2’\n",
            "\n",
            "summary.csv.2       100%[===================>] 318.70K   399KB/s    in 0.8s    \n",
            "\n",
            "2024-02-12 04:21:40 (399 KB/s) - ‘summary.csv.2’ saved [326352/326352]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget file.weasoft.com/images.zip\n",
        "!wget file.weasoft.com/summary.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget file.weasoft.com/256.pt -O 256.pt\n",
        "!unzip images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weVRt8uyGkJS",
        "outputId": "fea76419-8fd9-4508-eb62-32fffe98219b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-12 04:21:40--  http://file.weasoft.com/256.pt\n",
            "Resolving file.weasoft.com (file.weasoft.com)... 149.28.13.194\n",
            "Connecting to file.weasoft.com (file.weasoft.com)|149.28.13.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153353860 (146M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘256.pt’\n",
            "\n",
            "256.pt               58%[==========>         ]  85.11M  17.7MB/s    eta 4s     ^C\n",
            "Archive:  images.zip\n",
            "replace images/10952_050583472155929066.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "eff = torchvision.models.efficientnet_v2_s()\n",
        "mynet = eff.features\n",
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eff = mynet.to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    return features"
      ],
      "metadata": {
        "id": "PxjIy7A2GqBE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "1daf-m55Gu3-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "molecules_rows = pd.read_csv(\"summary.csv\")"
      ],
      "metadata": {
        "id": "yUKBiGWGb0yh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, deepsmiles, os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "cids = list(molecules_rows[\"cid\"])\n",
        "train_cids = [i for i in cids if not i in [286, 6587, 6562, 11203]]\n",
        "# val_cids = [\"a\"]\n",
        "val_cids = [286, 6587, 6562, 11203]\n",
        "\n",
        "train_names = [i for i in os.listdir(\"./images\") if int(i.split(\"_\")[0]) in train_cids]\n",
        "val_names = [i for i in os.listdir(\"./images\") if int(i.split(\"_\")[0]) in val_cids]\n",
        "from PIL import ImageOps\n",
        "BATCH_SIZE = 16\n",
        "def getitem(index, train=True):\n",
        "  global val_names, train_names\n",
        "  Y = []\n",
        "  imgs = []\n",
        "\n",
        "  # train=FalseTAMADE WEISM TRAIN = FALSE!!!guaibude bunengfanhua shouzhima ghuainbbude val namehao zuibakouke azhiqianweismkeyile? jiushiyingweimeiyouyongnamesle\n",
        "  names = train_names if train else val_names\n",
        "  start = index*BATCH_SIZE\n",
        "  end =  (index+1)*BATCH_SIZE\n",
        "  end = len(names) if end>len(names) else end\n",
        "  # print(start, end)\n",
        "  for i in range(start, end):\n",
        "\n",
        "\n",
        "    name = random.choice([i for i in names])\n",
        "    # name = random.choice([i for i in os.listdir(\"./images\")]) #shetoudzikunhzhegegebuhaishisuijidemaguaibude\n",
        "\n",
        "    img = Image.open(f\"images/{name}\").convert(\"RGB\")\n",
        "    if train:\n",
        "      img = img.rotate(random.choice([0,90,180,270]), expand=0).resize((400,400))\n",
        "      if random.random()>0.5:\n",
        "        img = ImageOps.flip(img)\n",
        "      if random.random()>0.5:\n",
        "        img = ImageOps.mirror(img)\n",
        "      img.crop((random.random()*30,random.random()*30,400-random.random()*30,400-random.random()*30))\n",
        "    img = np.array(img.resize((400,400)), dtype=\"float32\")\n",
        "    if train:\n",
        "      # img[:,:,0] *= random.random()*0.2+0.9\n",
        "      img[:,:,1] *= random.random()*0.1+0.9\n",
        "      img[:,:,2] *= random.random()*0.1+0.9\n",
        "      img[:,:,:] *= random.random()*0.1+0.8\n",
        "    img = torch.tensor(np.array(img)).permute(2, 0, 1).to(torch.float32).to(device)\n",
        "    # if train:\n",
        "    #   # img+=torch.rand((3,400,400)).to(device)*50\n",
        "    #   for _ in range(30):\n",
        "    #     x,y = int(random.random()*380),int(random.random()*380)\n",
        "    #     img[:,x:x+20, y:y+20] = torch.rand((3,20,20))*250\n",
        "#  guoranhaishiyaoqudizokunduzikoue\n",
        "    # if train:\n",
        "      # img*=torch.round(torch.tanh(5*torch.rand((3,400,400)))).to(device)\n",
        "    # print(name)\n",
        "    # print(converte//r.encode(molecules_rows[molecules_rows[\"cid\"]==int(name.split(\"_\")[0])][\"canonicalsmiles\"].item()))\n",
        "    length = len(converter.encode(molecules_rows[molecules_rows[\"cid\"]==int(name.split(\"_\")[0])][\"canonicalsmiles\"].item()))\n",
        "    Y.append(length)\n",
        "    imgs.append(img)\n",
        "  return torch.stack(imgs), Y\n"
      ],
      "metadata": {
        "id": "CCWejvj5I0n4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.decoder.encoder_dim = torch.nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, image, text_in, xmask):\n",
        "    image_feature = self.encoder(image)\n",
        "    out = self.decoder(text_in, image_feature, x_mask=xmask)\n",
        "    return out\n",
        "\n",
        "\n",
        "model = torch.load(\"drive/MyDrive/encoder\", map_location=device) #it is model not dict\n"
      ],
      "metadata": {
        "id": "UcwGiXx4c-ZT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class head(torch.nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.eff = model.eff\n",
        "    self.pooling = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
        "    self.projection = torch.nn.Linear(1280,16).to(device)\n",
        "    self.projection3 = torch.nn.Linear(16,1).to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    features = torch.nn.functional.relu(self.projection(self.pooling(features).squeeze(-1).squeeze(-1)))\n",
        "    features = torch.nn.functional.relu(self.projection3(features))\n",
        "    return features"
      ],
      "metadata": {
        "id": "OTpcJJo1eUKo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = head(model)"
      ],
      "metadata": {
        "id": "Zd4IrcQ4emcH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val():\n",
        "  global val_names\n",
        "  np.random.shuffle(val_names)\n",
        "  running_loss = 0\n",
        "  eval_mod = mod.train(False).to(device)\n",
        "\n",
        "  image, Y = getitem(0, train=False)\n",
        "\n",
        "  image = image.to(device)\n",
        "\n",
        "\n",
        "  outputs = eval_mod(image)\n",
        "  loss = torch.nn.functional.mse_loss(outputs, torch.tensor(Y).to(device).to(torch.float32).unsqueeze(-1))\n",
        "\n",
        "  return loss.item()\n"
      ],
      "metadata": {
        "id": "p3ffOOZ7glKU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(lr, gamma, epoch, forzen, reload=False, split=1, pretrain=\"mol_80k\"):\n",
        "  global mod\n",
        "  # lr = lr*split #oh why learning rate*4 this is why it trains much better, still need nigher learning rate zuibagankouke zhege haizaizheli a a banbenwanquanluanle\n",
        "  if reload:\n",
        "    model = torch.load(\"drive/MyDrive/encoder\", map_location=device) #it is model not dict\n",
        "    mod = head(model)\n",
        "\n",
        "  losses = []\n",
        "  access = []\n",
        "  val_acc = []\n",
        "  val_loss = []\n",
        "  optimizer = torch.optim.AdamW(\n",
        "    mod.parameters(),\n",
        "    lr=lr)\n",
        "  import pylab\n",
        "  scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
        "  running_loss = 0\n",
        "  for epoch in range(epoch):\n",
        "    np.random.shuffle(train_names)\n",
        "    mod = mod.train(True)\n",
        "\n",
        "    for i in range(len(train_names)//(BATCH_SIZE*split)):\n",
        "      if i==None:\n",
        "        continue\n",
        "      mod = mod.train(True)\n",
        "      start_index = i\n",
        "      image, Y = getitem(i, True)\n",
        "      image = image.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mod(image)\n",
        "      loss = torch.nn.functional.mse_loss(outputs, torch.tensor(Y).to(device).to(torch.float32).unsqueeze(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "      if i%5==0:\n",
        "        a = val()\n",
        "        print(a)\n",
        "        val_loss.append(a)\n",
        "        scheduler.step()\n",
        "  pylab.plot([i.item() for i in access], label=\"Train\")\n",
        "  pylab.plot(np.arange(1, len(val_acc)+1)*(len(access)/len(val_acc)),val_acc,label=\"Val\")\n",
        "  pylab.legend()\n",
        "  pylab.show()\n",
        "  return mod, access, losses, val_acc, val_loss"
      ],
      "metadata": {
        "id": "5MOz-LqhgKio"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(0.001, 0.972, 6,  True, reload=True, pretrain=\"mol_80k\") #yeah need 0.001, if too smalel 0.001 cannot be trained duzikunduziex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "OBBr3uiRg5We",
        "outputId": "41e0198c-3a6e-40b7-ddae-24591c6f4974"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82.02108764648438\n",
            "42.68946075439453\n",
            "16.040904998779297\n",
            "23.619922637939453\n",
            "13.539985656738281\n",
            "6.694955348968506\n",
            "12.572580337524414\n",
            "19.423480987548828\n",
            "4.806560039520264\n",
            "18.29102325439453\n",
            "18.542097091674805\n",
            "15.016664505004883\n",
            "9.664754867553711\n",
            "6.551578521728516\n",
            "10.105225563049316\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-203d0973428d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.972\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mol_80k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-3204ae0330bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lr, gamma, epoch, forzen, reload, split, pretrain)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bbd8cce40268>\u001b[0m in \u001b[0;36mgetitem\u001b[0;34m(index, train)\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# if train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#   # img+=torch.rand((3,400,400)).to(device)*50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "mod = mod.cpu()\n",
        "model = model.cpu()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "j4zfCNUWihZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}