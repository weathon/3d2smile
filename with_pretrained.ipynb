{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/with_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ITeA6EKkAF"
      },
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ApJFGgEHvGr",
        "outputId": "76d648ef-4df2-43ec-f7de-c4b9e4cd9d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torchview in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pylab\n",
        "import pandas\n",
        "import sys\n",
        "from PIL import Image\n",
        "!pip install graphviz\n",
        "!pip install torchview\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CPm75AN7KwuT"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    HOME_DIR = \"/content\"\n",
        "else:\n",
        "    HOME_DIR = \"/arc/project/st-dushan20-1/rendered\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbUsttIrsHvJ",
        "outputId": "9eabac7f-971f-4731-a507-f467275827f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'IUPAC2Struct' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sergsb/IUPAC2Struct.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9AZq2vcKKp1T"
      },
      "outputs": [],
      "source": [
        "sys.path.append(f\"{HOME_DIR}/IUPAC2Struct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5OTc2WSng04e"
      },
      "outputs": [],
      "source": [
        "!ls drive/MyDrive | grep iupav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTUvAf2LIvE",
        "outputId": "67296bb4-5ce3-47f4-dcd2-ca732ded6029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.Transformer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.EncoderLayer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.MultiHeadAttention' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.MLP' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.Decoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.DecoderLayer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:888: SourceChangeWarning: source code of class 'transformer.PositionalEncoding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "if IN_COLAB:\n",
        "    os.system('cp drive/MyDrive/WWVSB/iupac2smiles_model.pt .')\n",
        "    # os.system('cp \"drive/MyDrive/iupac2smiles_model.py\" .') wtf stupid\n",
        "\n",
        "M = torch.load(f\"{HOME_DIR}/iupac2smiles_model.pt\", map_location=device)\n",
        "M.device = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YEUiZAALMjD",
        "outputId": "1a0bea8a-5f6e-451c-b542-2186334fd396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27, 27, 8, 23, 38, 9, 37, 54, 14, 54, 54, 54, 8, 38, 9, 54, 54, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "M.tgt_model.encode(\"CC(=O)Nc1ccc(O)cc1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kGBgp-IELV8T"
      },
      "outputs": [],
      "source": [
        "smiles_tokenlizer = M.tgt_model.encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX8W8av74hzg",
        "outputId": "d710bf61-a20d-4b0c-f0e9-29703a335fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6E20RUMJLFP"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O1oqSyv1JSBX"
      },
      "outputs": [],
      "source": [
        "\n",
        "if IN_COLAB:\n",
        "  if not \"80k\" in \"\".join(os.listdir(\"/content\")):\n",
        "    os.system(\"cp drive/MyDrive/80k.zip .\")\n",
        "    os.system(\"cp drive/MyDrive/80k.csv .\")\n",
        "    os.system(\"unzip 80k.zip\")\n",
        "    HOME_DIR = \"/content\"\n",
        "else:\n",
        "  if not \"80k\" in \"\".join(os.listdir(\"/arc/project/st-dushan20-1/rendered\")):\n",
        "    os.system(\"wget file.weasoft.com/80k.zip\")\n",
        "    os.system(\"wget http://file.weasoft.com/80k.csv\")\n",
        "    os.system(\"unzip 80k.zip\")\n",
        "    HOME_DIR = \"/arc/project/st-dushan20-1/rendered\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcmQxVtjJwA1",
        "outputId": "aac83b43-4518-4dea-f867-c92148bdfdcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['cid', 'cmpdname', 'cmpdsynonym', 'mw', 'mf', 'polararea', 'complexity',\n",
              "       'xlogp', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'inchi',\n",
              "       'isosmiles', 'canonicalsmiles', 'inchikey', 'iupacname', 'exactmass',\n",
              "       'monoisotopicmass', 'charge', 'covalentunitcnt', 'isotopeatomcnt',\n",
              "       'totalatomstereocnt', 'definedatomstereocnt', 'undefinedatomstereocnt',\n",
              "       'totalbondstereocnt', 'definedbondstereocnt', 'undefinedbondstereocnt',\n",
              "       'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'neighbortype', 'meshheadings',\n",
              "       'annothits', 'annothitcnt', 'aids', 'cidcdate', 'sidsrcname', 'depcatg',\n",
              "       'annotation'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import os\n",
        "ids = [i.split(\"_\")[0] for i in os.listdir(f\"{HOME_DIR}/rendered\")]\n",
        "\n",
        "with open(\"ids.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(list(set(ids))))\n",
        "\n",
        "import pandas as pd\n",
        "csv = pd.read_csv(\"80k.csv\")\n",
        "cids = csv[\"cid\"]\n",
        "csv.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcv_YUPfMJup"
      },
      "outputs": [],
      "source": [
        "Ys = {}\n",
        "invalid_cids = []\n",
        "for i in cids.values:\n",
        "  try:\n",
        "    tmp = smiles_tokenlizer(csv[csv[\"cid\"] == i][\"canonicalsmiles\"].values[0])\n",
        "    if not tmp == None:\n",
        "      Ys[i] = tmp\n",
        "    else:\n",
        "      smiles_tokenlizer.append(i)\n",
        "\n",
        "  except:\n",
        "    invalid_cids.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY0Vb-37LmUL"
      },
      "outputs": [],
      "source": [
        "if len(invalid_cids) == 0:\n",
        "  print(\"OOHH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18OLQ2quPx4i"
      },
      "outputs": [],
      "source": [
        "example_in = Image.open(f\"{HOME_DIR}/rendered/6912034_0.jpg\")\n",
        "example_out = csv[csv[\"cid\"]==6912034][\"canonicalsmiles\"].values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMaoGQcqQE4x"
      },
      "outputs": [],
      "source": [
        "example_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZMYTdkHL1M7"
      },
      "outputs": [],
      "source": [
        "M.tgt_model.decode(Ys[6912034])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq = [1] * 72\n",
        "for i in Ys.keys():\n",
        "  for j in Ys[i]:\n",
        "    freq[j]+=1\n",
        "\n",
        "reverse_freq = 1/np.array(freq)\n",
        "pylab.plot(freq)"
      ],
      "metadata": {
        "id": "0qdKCP30J9jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_freq_ = reverse_freq**0.5\n",
        "scales = np.mean(reverse_freq_)\n",
        "np.mean(reverse_freq_/scales), np.min(reverse_freq_/scales)"
      ],
      "metadata": {
        "id": "afmBgg2OL6tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.plot(reverse_freq_/scales)"
      ],
      "metadata": {
        "id": "BhotsWcyKcNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tensor(reverse_freq_/scales).to(device)"
      ],
      "metadata": {
        "id": "kpL7nkOaMgno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ropUGNpMaWQ"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAjiWuU_MboG"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, size, downsampling=True):\n",
        "    super().__init__()\n",
        "    self.Conv2D = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(in_channels, out_channels, (3,3), stride=2 if downsampling else 1, padding=(3 if size%2==0 else 2) if downsampling else 2),\n",
        "        torch.nn.BatchNorm2d(out_channels),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Conv2d(out_channels, out_channels, (3,3)),\n",
        "        torch.nn.BatchNorm2d(out_channels),\n",
        "    )\n",
        "    self.project = torch.nn.Conv2d(in_channels, out_channels, (1,1))\n",
        "    self.pooling = torch.nn.AvgPool2d((2,2)) if downsampling else torch.nn.Identity()\n",
        "    self.relu = torch.nn.ReLU()\n",
        "  def forward(self, images):\n",
        "    x = self.Conv2D(images)\n",
        "    images = self.pooling(self.project(images))\n",
        "    return self.relu(x+images)\n",
        "resblock = ResNetBlock(3,12,400, 0)\n",
        "inp_img = torch.permute(torch.tensor(np.expand_dims(example_in, 0).astype(\"float32\")), (0,3,1,2))\n",
        "oup = resblock(inp_img)[:,-3:,:,:]\n",
        "pylab.imshow(torch.permute(oup, (0,2,3,1)).cpu().detach().numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfox5CWOR8aE"
      },
      "outputs": [],
      "source": [
        "torch.permute(torch.tensor(np.expand_dims(example_in, 0).astype(\"float32\")), (0,3,1,2)).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTG6F8JOYFlT"
      },
      "outputs": [],
      "source": [
        "t = torch.randn(1,64,400,400)\n",
        "torch.flatten(t)\n",
        "torch.flatten(t, start_dim=2,end_dim=3).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qfnGU9PWGwn"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self, channels_list, heads, dropout, orginal_size):\n",
        "    super().__init__()\n",
        "    self.conv2d = torch.nn.Sequential(\n",
        "        # torch.nn.Conv2d(3, 32, (7, 7), stride=3),\n",
        "        # torch.nn.AvgPool2d((3,3), padding=1)\n",
        "        torch.nn.Conv2d(3, 32, (7, 7), stride=2, padding=1),\n",
        "        torch.nn.MaxPool2d((3, 3), stride=2)\n",
        "    ).to(device)\n",
        "\n",
        "    self.resnet_blocks = []\n",
        "    for i in range(len(channels_list)-1):\n",
        "      self.resnet_blocks.append(ResNetBlock(channels_list[i], channels_list[i], size=66//(2**i)).to(device))\n",
        "      self.resnet_blocks.append(ResNetBlock(channels_list[i], channels_list[i+1], size=66//(2**i), downsampling=0).to(device)) #gangcai self not defined\n",
        "\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(512,1024),\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Linear(1024, 512),\n",
        "        torch.nn.GELU(),\n",
        "    ).to(device)\n",
        "    self.posem = torch.nn.Embedding(36, 512).to(device)\n",
        "    self.mha = torch.nn.MultiheadAttention(channels_list[-1], heads, dropout = 0.1).to(device)\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.conv2d(images)\n",
        "    for f in self.resnet_blocks:\n",
        "      features = f(features)\n",
        "    features = torch.flatten(features, start_dim=2, end_dim=3)\n",
        "    features = torch.permute(features, (0, 2, 1))\n",
        "    ran = torch.arange(0, 36)\n",
        "    pos = self.posem(ran.to(device).unsqueeze(0))\n",
        "    features = pos + features#.clone()\n",
        "    att = self.mha(features, features, features, need_weights=False)[0]\n",
        "    return self.mlp(att) + features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_yQXETC4Aso"
      },
      "outputs": [],
      "source": [
        "# posem = torch.nn.Embedding(36, 512)\n",
        "# posem(torch.range(0, 36).unsqueeze(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf72lcreXHXc"
      },
      "outputs": [],
      "source": [
        "NUM_HEADS = 8\n",
        "CHANNELS = [32, 64, 128, 256, 512]\n",
        "DROPOUT = 0.1\n",
        "inp_img = inp_img.to(device)\n",
        "encoder = ImageEncoder(CHANNELS, NUM_HEADS, DROPOUT, 400)\n",
        "encoder(inp_img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSOW82wLje_j"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in encoder.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dJ-B1oNdNTm"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(encoder, (1,3,400,400), depth=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozo8Eyvn6hHD"
      },
      "outputs": [],
      "source": [
        "from torchview import draw_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWEgEfvK6vv-"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "graphviz.set_jupyter_format('png')\n",
        "encoder.to(device)\n",
        "model_graph = draw_graph(encoder, input_size=(1, 3, 400, 400), depth=5, device=device)\n",
        "# model_graph.visual_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7qYoChlkQip"
      },
      "source": [
        "# Connect with pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etzBYtUnkU2y"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask(tgt_mask):\n",
        "    size = tgt_mask.size(-1)\n",
        "    return tgt_mask.to(torch.uint8) & torch.tril(torch.ones(1,size,size, dtype=torch.uint8)).to(tgt_mask.device)\n",
        "\n",
        "def pad_pack(sequences):\n",
        "    maxlen = max(map(len, sequences))\n",
        "    batch = torch.LongTensor(len(sequences),maxlen).fill_(0)\n",
        "    for i,x in enumerate(sequences):\n",
        "        batch[i,:len(x)] = torch.LongTensor(x)\n",
        "    return batch\n",
        "\n",
        "def find_first_working(x):\n",
        "  for i in x:\n",
        "    if i<=71:\n",
        "      return i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlOwZ8Bck4HJ"
      },
      "outputs": [],
      "source": [
        "inp = M.src_model.encode(\"N-(4-hydroxyphenyl)acetamide\")\n",
        "src = torch.tensor(pad_pack([inp]), device=(M.device))\n",
        "src_mask = (src != 0).unsqueeze(-2).to(M.device)#.shape\n",
        "latent = M.T.encoder(M.T.src_embedder(src), (src != 0).unsqueeze(-2).to(M.device))\n",
        "l = latent.to(torch.device(\"cpu\"))\n",
        "l.cpu().detach().numpy()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FIduYubkZu8"
      },
      "outputs": [],
      "source": [
        "padded_tgt = pad_pack([[2]]).to(M.device)\n",
        "empty_start = M.T.tgt_embedder(padded_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW1DXfXOuDkh"
      },
      "outputs": [],
      "source": [
        "inp_img = inp_img.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PBJFBlLu1rX"
      },
      "outputs": [],
      "source": [
        "inp_img.is_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAYA64o5uHJI"
      },
      "outputs": [],
      "source": [
        "encoder = encoder.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd2aO18WukCQ"
      },
      "outputs": [],
      "source": [
        "encoder = encoder.to(torch.device('cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTV-JYxsurxX"
      },
      "outputs": [],
      "source": [
        "next(M.T.decoder.parameters()).is_cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4ugwGJBjeL0"
      },
      "outputs": [],
      "source": [
        "latent = encoder(inp_img)\n",
        "src_mask = (torch.zeros(len(latent[0]))).unsqueeze(-2).to(M.device)#.shape\n",
        "out = M.T.decoder(empty_start, latent, src_mask, subsequent_mask((padded_tgt != 0).unsqueeze(-2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TneB6GCGnuUr"
      },
      "outputs": [],
      "source": [
        "# Test for teaching forcing\n",
        "# forcing = pad_pack([[2]+M.tgt_model.encode(\"CC(=O)Nc1ccc(O)cc\")]).to(M.device)\n",
        "forcing = pad_pack([[2,0]]).to(device)\n",
        "forcing_start = M.T.tgt_embedder(forcing).to(device)\n",
        "\n",
        "latent = encoder(inp_img)\n",
        "src_mask = torch.zeros(36).unsqueeze(-2).to(device)\n",
        "out = M.T.decoder(forcing_start, latent, src_mask, subsequent_mask((forcing != 0).unsqueeze(-2)))\n",
        "# out = M.T.decoder(forcing_start, l, src_mask, subsequent_mask((forcing != 0).unsqueeze(-2)))\n",
        "out = M.T.generator(out)\n",
        "print (out.shape)\n",
        "ans = []\n",
        "for i in range(2):\n",
        "  ans.append(torch.sort(out[0], dim=1,\n",
        "                        descending=True)[1].cpu().detach().numpy()[i][0])\n",
        "M.tgt_model.decode(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_bkcC5c4EmG"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "for i in M.T.decoder.parameters():\n",
        "  i.requires_grad = False\n",
        "\n",
        "for i in M.T.generator.parameters():\n",
        "  i.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in M.T.decoder.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "WjievIiBFj0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9--kazs49Lq"
      },
      "outputs": [],
      "source": [
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, embeding, decoder, generator):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.embeding = embeding\n",
        "    self.decoder = decoder\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward (self, image, text_in):\n",
        "    image_feature = self.encoder(image)\n",
        "    paded = pad_pack(text_in).to(device)\n",
        "    embedded = self.embeding(paded)\n",
        "    out = self.decoder(embedded, image_feature,\n",
        "                       torch.zeros(36).unsqueeze(-2).to(M.device),\n",
        "                       subsequent_mask((paded != 0)\n",
        "                       .unsqueeze(-2)))\n",
        "    out = self.generator(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLLhmAq_AUrV"
      },
      "outputs": [],
      "source": [
        "class SMILESGenerator(torch.nn.Module):\n",
        "  def __init__(self, encoder, embeding, decoder, generator, max_len):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.embeding = embeding\n",
        "    self.decoder = decoder\n",
        "    self.generator = generator\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def forward (self, image, text_in):\n",
        "    image_feature = self.encoder(image)\n",
        "    for i in range(self.max_len):\n",
        "      paded = pad_pack(text_in).to(device)\n",
        "      embedded = self.embeding(paded)\n",
        "      out = self.decoder(embedded, image_feature,\n",
        "                        torch.zeros(36).unsqueeze(-2).to(M.device),\n",
        "                        subsequent_mask((paded != 0)\n",
        "                        .unsqueeze(-2)))\n",
        "      out = self.generator(out)\n",
        "      next = torch.sort(out, descending=True)[1][0,0].cpu().detach().numpy()[0] #forgot descending\n",
        "      if next == 3:\n",
        "        break\n",
        "      text_in[0] += [next]\n",
        "      # print(text_in)\n",
        "    return M.tgt_model.decode(text_in[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsErNlKWLIU4"
      },
      "outputs": [],
      "source": [
        "model = Image2SMILES(encoder.to(device),\n",
        "    M.T.tgt_embedder.to(device),\n",
        "             M.T.decoder.to(device), M.T.generator.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_nztT0SA2bF"
      },
      "outputs": [],
      "source": [
        "gen = SMILESGenerator(encoder,\n",
        "    M.T.tgt_embedder,\n",
        "             M.T.decoder, M.T.generator, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaa6Q7N-EFh0"
      },
      "outputs": [],
      "source": [
        "torch.argmax(model(inp_img, [[2, 0]])[0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGxLhbxdEaki"
      },
      "outputs": [],
      "source": [
        "gen(inp_img, [[2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMu3ynZDmB7"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "  t = np.exp(x)\n",
        "  return t/np.sum(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HSOkByEAJtH"
      },
      "outputs": [],
      "source": [
        "pylab.plot(softmax(model(inp_img, [[2, 0]])[0][0].cpu().detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZBKvGoDvPy"
      },
      "outputs": [],
      "source": [
        "M.tgt_model.decode([37])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUL3_0KTE181"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model. parameters() if not p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKzJ8AccE2QM"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwmD_bfpGxuH"
      },
      "outputs": [],
      "source": [
        "lf = torch.nn.CrossEntropyLoss(label_smoothing=0.1, reduction=\"none\")\n",
        "def loss_fn(pred, truth):\n",
        "  mask = truth != 0\n",
        "  pred = pred.permute(0,2,1) #WHY\n",
        "  truth = torch.nn.functional.one_hot(truth, num_classes=72).type(torch.float32).permute(0,2,1)\n",
        "  l = lf(pred, truth)\n",
        "  return torch.sum(mask*l)/torch.sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cDfMsedBG3I"
      },
      "outputs": [],
      "source": [
        "def saveloss(loss_list):\n",
        "  pylab.scatter(np.arange(len(loss_list)), loss_list)\n",
        "  pylab.plot(np.arange(len(loss_list)), loss_list)\n",
        "  pylab.savefig(\"loss.png\")\n",
        "  with open(\"loss.txt\",\"w\") as f:\n",
        "    f.write(\"\\n\".join([str(i) for i in loss_list]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters() if not p.requires_grad)"
      ],
      "metadata": {
        "id": "ecMTsgaoFKV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbTaSobFGH_n"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "files = os.listdir(\"rendered\")\n",
        "import multiprocessing\n",
        "\n",
        "data_reader, trainer = multiprocessing.Pipe()\n",
        "\n",
        "def getitem(index):\n",
        "  start_index = index * BATCH_SIZE\n",
        "  Xs_img = []\n",
        "  Xs_text = []\n",
        "  y = [] #This is slow, rewrite later\n",
        "  for _ in range(BATCH_SIZE):\n",
        "    index = start_index + _\n",
        "    try:\n",
        "      id = int(files[index].split(\"_\")[0])\n",
        "    except:\n",
        "      break\n",
        "    index = start_index + _\n",
        "    img = np.array(Image.open(f\"rendered/{files[index]}\").rotate(np.random.uniform(0,360), expand = 1).resize((400,400)), dtype=\"float32\")\n",
        "    noise = np.random.uniform(size=img.shape)*20\n",
        "    img += noise\n",
        "    Xs_img.append(img)\n",
        "    Xs_text.append([2] + Ys[id])\n",
        "    y.append(Ys[id] + [3])\n",
        "  Xs_img = torch.permute(torch.tensor(np.array(Xs_img)), (0,3,1,2))\n",
        "  data_reader.send(([Xs_img, pad_pack(Xs_text)], pad_pack(y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrpDDJ2jE3Hs",
        "outputId": "dfc019e1-ddfc-4f47-8c92-46b401722411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n"
          ]
        }
      ],
      "source": [
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)\n",
        "\n",
        "\n",
        "\n",
        "loss_list = []\n",
        "model.to(device)\n",
        "\n",
        "getitem(0)\n",
        "for epoch in range(30):\n",
        "  np.random.shuffle(files)\n",
        "  print('EPOCH {}:'.format(epoch + 1))\n",
        "  model.train(True)\n",
        "  running_loss = 0.\n",
        "  last_loss = 0.\n",
        "  for i in range(len(files)//BATCH_SIZE):\n",
        "    if i != len(files)//BATCH_SIZE:\n",
        "      p = multiprocessing.Process(target=getitem, args=(i+1,))\n",
        "      p.start()\n",
        "    if not trainer.poll():\n",
        "      print(\"WARNING: reading too slow\")\n",
        "\n",
        "    (image, text_in), text_out = trainer.recv()\n",
        "    image = image.to(device)\n",
        "    text_out = text_out.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(image, text_in)\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i%10 == 9:\n",
        "      print(f\"Training loss: {running_loss/10}\") #first time loss is small because it is dived by 10 where there is only 1\n",
        "      loss_list.append(running_loss/10)\n",
        "      saveloss(loss_list)\n",
        "      running_loss = 0.\n",
        "    if i%40 == 0:\n",
        "      print(f\"Example Output: {gen(inp_img, [[2]])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRojMPmiBzNQ"
      },
      "outputs": [],
      "source": [
        "_ = list(map(lambda x: np.exp(-0.1*x)+np.random.normal()*0.001*x, list(range(100))))\n",
        "saveloss(_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLqBA46hLO50"
      },
      "outputs": [],
      "source": [
        "outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnq92w-lLcq-"
      },
      "outputs": [],
      "source": [
        "torch.nn.functional.one_hot(data[1], num_classes=72).size()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1UCIRVi_vKSRZf69Xa6ZqjaUo-_ryLvQP",
      "authorship_tag": "ABX9TyMKcCPsFobiV3ZdOfFgSzbA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}