{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2goMTtD+KFUGhM5aqGVnU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qquj2I2RlAm"
      },
      "outputs": [],
      "source": [
        "!wget file.weasoft.com/images.zip\n",
        "!wget file.weasoft.com/summary.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip images.zip"
      ],
      "metadata": {
        "id": "7nIJVhnhTfXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget file.weasoft.com/29_4010.pt"
      ],
      "metadata": {
        "id": "3XXkX-uNSepf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b2ebed-d658-491a-afc1-c660ea52e313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 00:42:59--  http://file.weasoft.com/29_4010.pt\n",
            "Resolving file.weasoft.com (file.weasoft.com)... 149.28.13.194\n",
            "Connecting to file.weasoft.com (file.weasoft.com)|149.28.13.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149608395 (143M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘29_4010.pt’\n",
            "\n",
            "29_4010.pt          100%[===================>] 142.68M  18.9MB/s    in 8.7s    \n",
            "\n",
            "2024-01-24 00:43:08 (16.4 MB/s) - ‘29_4010.pt’ saved [149608395/149608395]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/suanfaxiaohuo/SwinOCSR.git\n",
        "!pip install focal_loss_torch wandb\n",
        "!pip3 install deepsmiles yacs tqdm"
      ],
      "metadata": {
        "id": "mDo6zTHeS4Om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcde1aa6-0b09-4122-c6ba-f35406c0e61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: deepsmiles, yacs\n",
            "Successfully installed deepsmiles-1.0.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8rc_IdNTb0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import deepsmiles\n",
        "from typing import Any, cast, Callable, List, Tuple, Union\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nNBpujPhUwPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./SwinOCSR/model/Swin-transformer-focalloss\")\n",
        "sys.path.append(\"./SwinOCSR/model/\")\n",
        "from pre_transformer import Transformer\n",
        "class FocalLossModelInference:\n",
        "    \"\"\"\n",
        "    Inference Class\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Load dictionary that maps tokens to integers\n",
        "        word_map_path = './SwinOCSR/Data/500wan/500wan_shuffle_DeepSMILES_word_map'\n",
        "        self.word_map = torch.load(word_map_path)\n",
        "        self.inv_word_map = {v: k for k, v in self.word_map.items()}\n",
        "\n",
        "        # Define device, load models and weights\n",
        "        self.dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # self.args, config = self.get_inference_config()\n",
        "        # self.encoder = build_model(config, tag=False)\n",
        "        self.decoder = self.build_decoder()\n",
        "        # self.load_checkpoint(\"./swin_transform_focalloss.pth\")\n",
        "        self.decoder = self.decoder.to(self.dev).eval()\n",
        "        # self.encoder = self.encoder.to(self.dev).eval()\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load checkpoint and update encoder and decoder accordingly\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path (str): path of checkpoint file\n",
        "        \"\"\"\n",
        "        print(f\"=====> {checkpoint_path} <=====\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        # encoder_msg = self.encoder.load_state_dict(checkpoint['encoder'],\n",
        "        #                                            strict=False)\n",
        "        decoder_msg = self.decoder.load_state_dict(checkpoint['decoder'],\n",
        "                                                   strict=False)\n",
        "        # print(f\"Encoder: {encoder_msg}\")\n",
        "        print(f\"Decoder: {decoder_msg}\")\n",
        "        del checkpoint\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    def build_decoder(self):\n",
        "        \"\"\"\n",
        "        This method builds the Transformer decoder and returns it\n",
        "        \"\"\"\n",
        "        self.decoder_dim = 256  # dimension of decoder RNN\n",
        "        self.ff_dim = 2048\n",
        "        self.num_head = 8\n",
        "        self.dropout = 0.1\n",
        "        self.encoder_num_layer = 6\n",
        "        self.decoder_num_layer = 6\n",
        "        self.max_len = 277\n",
        "        self.decoder_lr = 5e-4\n",
        "        self.best_acc = 0.\n",
        "        return Transformer(dim=self.decoder_dim,\n",
        "                           ff_dim=self.ff_dim,\n",
        "                           num_head=self.num_head,\n",
        "                           encoder_num_layer=self.encoder_num_layer,\n",
        "                           decoder_num_layer=self.decoder_num_layer,\n",
        "                           vocab_size=len(self.word_map),\n",
        "                           max_len=self.max_len,\n",
        "                           drop_rate=self.dropout,\n",
        "                           tag=False)\n",
        "transformer_ = FocalLossModelInference()"
      ],
      "metadata": {
        "id": "OdtFJ4TKjyIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import pandas as pd\n",
        "import os\n",
        "# df = pd.read_csv(\"images_rows.csv\")\n",
        "# os.makedirs(\"images\",exist_ok=1)\n",
        "# def save_img(id):\n",
        "#   row = df[df[\"id\"]==id]\n",
        "#   print(row[\"image\"])\n",
        "#   with open(f\"images/{id}.png\", \"wb\") as fh:\n",
        "#       fh.write(base64.decodebytes(bytes(row[\"image\"].item().replace(\"data:image/png;base64,\",\"\"), \"utf-8\")))\n",
        "# df[\"id\"].apply(save_img)"
      ],
      "metadata": {
        "id": "nuWSq0NVS8Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./SwinOCSR/model/Swin-transformer-focalloss\")\n",
        "sys.path.append(\"./SwinOCSR/model/\")"
      ],
      "metadata": {
        "id": "yPrlAJRlWum_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "T2jSXpDvXC-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swin = torchvision.models.swin_t()\n",
        "swin.head = swin.avgpool = swin.flatten = torch.nn.Identity()\n",
        "mynet = swin\n",
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.eff = mynet.to(device)\n",
        "    self.projection = torch.nn.Linear(768,256).to(device)\n",
        "  def forward(self, images):\n",
        "    features = self.eff(images)\n",
        "    features = torch.flatten(features, start_dim=2, end_dim=3)\n",
        "    features = torch.permute(features, (0, 2, 1))\n",
        "    return self.projection(features)\n",
        "class Image2SMILES(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in, xmask):\n",
        "    image_feature = self.encoder(image)\n",
        "    out = self.decoder(text_in, image_feature, x_mask=xmask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "UBa_oqMlmBN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = torch.load(\"29_4010.pt\", map_location=device) #it is model not dict"
      ],
      "metadata": {
        "id": "6dA6xdVGXVog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_pack(sequences):\n",
        "    maxlen = max(map(len, sequences))\n",
        "    batch = torch.LongTensor(len(sequences),maxlen).fill_(0)\n",
        "    for i,x in enumerate(sequences):\n",
        "        batch[i,:len(x)] = torch.LongTensor(x)\n",
        "    return batch, maxlen"
      ],
      "metadata": {
        "id": "oW55KrXRYvOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SMILESGenerator(torch.nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text_in_, max_len, beam): #just changed beam=1 and it runs so beam cannot be random number???  yeah it has to been factor of 676, which is 2 2 13 13 but original paper did not use beam search\n",
        "    image_feature = self.encoder(image)\n",
        "    top_n = list([list(i) for i in torch.tensor(text_in_).repeat(beam,1).detach().cpu().numpy()])\n",
        "    # print(top_n)\n",
        "    image_feature = image_feature.repeat_interleave(beam, dim=0)#.repeat_interleave(beam)\n",
        "    for i in range(max_len):\n",
        "      if beam == 1:\n",
        "        padded_text, l = pad_pack(top_n)\n",
        "        padded_text = padded_text.to(device)\n",
        "        out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(l).to(device))\n",
        "        out = out[0,-1,:]\n",
        "        next = torch.argmax(out)\n",
        "        top_n[0] += [next.detach().cpu().item()]\n",
        "        if next == 78:\n",
        "          return top_n\n",
        "        continue\n",
        "      padded_text, l = pad_pack(top_n)\n",
        "      padded_text = padded_text.to(device)\n",
        "      out = self.decoder(padded_text, image_feature, x_mask=triangle_mask(l).to(device))\n",
        "      out = out[:,-1,:]\n",
        "      next = torch.topk(torch.flatten(out), beam)\n",
        "      indices2d = np.array(np.unravel_index(next.indices.cpu().numpy(), out.shape)).T\n",
        "      new_top_n = []\n",
        "      count = 0\n",
        "      for j in indices2d:\n",
        "        beam_number = j[0]\n",
        "        char = j[1]\n",
        "        if char == 78:\n",
        "          count += 1\n",
        "          new_top_n.append(top_n[beam_number])\n",
        "        else:\n",
        "          new_top_n.append(top_n[beam_number] + [char])\n",
        "      if count == beam:\n",
        "        return new_top_n\n",
        "      top_n = new_top_n\n",
        "    return top_n"
      ],
      "metadata": {
        "id": "HDyCwVzvX1zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = SMILESGenerator(mod.encoder, mod.decoder)"
      ],
      "metadata": {
        "id": "hHCjvssJYyzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def triangle_mask(size):\n",
        "    mask = 1- np.triu(np.ones((1, size, size)),k=1).astype('uint8')\n",
        "    mask = torch.autograd.Variable(torch.from_numpy(mask))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "lS-pBOLrZYAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget file.weasoft.com/reverse.map"
      ],
      "metadata": {
        "id": "SyP6kq3BbP9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b818b5-35df-447d-e517-193320a2cc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 00:43:45--  http://file.weasoft.com/reverse.map\n",
            "Resolving file.weasoft.com (file.weasoft.com)... 149.28.13.194\n",
            "Connecting to file.weasoft.com (file.weasoft.com)|149.28.13.194|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 875\n",
            "Saving to: ‘reverse.map’\n",
            "\n",
            "reverse.map         100%[===================>]     875  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-24 00:43:45 (125 MB/s) - ‘reverse.map’ saved [875/875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reversed_word_map = {}\n",
        "import json\n",
        "with open(\"reverse.map\",\"r\") as f:\n",
        "  reversed_word_map = json.loads(f.read())"
      ],
      "metadata": {
        "id": "CF9p_SoUZkI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from focal_loss.focal_loss import FocalLoss\n",
        "m = torch.nn.Softmax(dim=-1)\n",
        "lf = FocalLoss(gamma=2, ignore_index=0)#torch.nn.CrossEntropyLoss(label_smoothing=0.1, reduction=\"none\")\n",
        "def loss_fn(pred, truth):\n",
        "  pred = m(pred)\n",
        "  l = lf(pred, truth)\n",
        "  return l"
      ],
      "metadata": {
        "id": "rCtJABqeeSfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_acc(pred, truth):\n",
        "    pred = torch.argmax(pred, -1)\n",
        "    mask = truth != 0\n",
        "    match_case = truth == pred\n",
        "    return torch.sum(mask*match_case)/torch.sum(mask)"
      ],
      "metadata": {
        "id": "2le1E1LleajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1"
      ],
      "metadata": {
        "id": "bMR9YqMKebx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "molecules_rows = pd.read_csv(\"summary.csv\")"
      ],
      "metadata": {
        "id": "QXsYpnwnf00c"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(list(set([i.split(\"_\")[0] for i in os.listdir(\"images\")]))))"
      ],
      "metadata": {
        "id": "oI76DYvd-XFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "ids = molecules_rows[\"cid\"]\n",
        "def getitem(names, train=True):\n",
        "\n",
        "  ti = []\n",
        "  to = []\n",
        "  imgs = []\n",
        "  for i in range(16):\n",
        "    name = random.choice(names)\n",
        "    # if (train and name.startswith(\"11203\"))or (not train and not name.startswith(\"11203\")):\n",
        "    # if train == name.startswith(\"11203\"):\n",
        "    #   continue\n",
        "\n",
        "    img = Image.open(f\"images/{name}\").convert(\"RGB\")\n",
        "    if train:\n",
        "      img = img.rotate(random.choice([0,90,270]), expand=0).resize((400,400))\n",
        "      img.crop((random.random()*20,random.random()*20,400-random.random()*20,400-random.random()*20))\n",
        "    img = np.array(img.resize((400,400)), dtype=\"float32\")\n",
        "    if train:\n",
        "      img[:,:,0] *= random.random()*0.2+0.9\n",
        "      img[:,:,1] *= random.random()*0.2+0.9\n",
        "      img[:,:,2] *= random.random()*0.2+0.9\n",
        "    img = torch.tensor(np.array(img)).permute(2, 0, 1).to(torch.float32).to(device)\n",
        "    if train:\n",
        "      img*=torch.round(torch.tanh(5*torch.rand((3,400,400)))).to(device)\n",
        "    # print(name)\n",
        "    # print(converte//r.encode(molecules_rows[molecules_rows[\"cid\"]==int(name.split(\"_\")[0])][\"canonicalsmiles\"].item()))\n",
        "    smiles = [transformer_.word_map[i] for i in converter.encode(molecules_rows[molecules_rows[\"cid\"]==int(name.split(\"_\")[0])][\"canonicalsmiles\"].item())]\n",
        "    ti.append([77] + smiles)\n",
        "    to.append(smiles + [78])\n",
        "    imgs.append(img)\n",
        "  return torch.stack(imgs), ti, to\n"
      ],
      "metadata": {
        "id": "KOBdK4giehsf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack([torch.tensor([1,2]) for i in range(10)])"
      ],
      "metadata": {
        "id": "cTf4Es8m8bSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pylab\n",
        "pylab.imshow(getitem(os.listdir(\"images\")[0])[0].cpu().permute(0, 2, 3, 1).numpy()[0]/255)"
      ],
      "metadata": {
        "id": "BZIs_57DUPLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "molecules_rows"
      ],
      "metadata": {
        "id": "vSXOzMle993e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"\\n\".join([str(i) for i in df[\"cid\"].unique()]))"
      ],
      "metadata": {
        "id": "jWuHtPLbf2U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in mod.decoder.parameters():\n",
        "  i.require_grad = True"
      ],
      "metadata": {
        "id": "pF-0FNSUjPQO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val():\n",
        "  running_loss = 0\n",
        "  mod.train(False)\n",
        "  ids = [i for i in os.listdir(\"images\") if i.startswith(\"11203\")]\n",
        "  # ids = [i if i.startswith(\"11203\") else None for i in os.listdir(\"images\")]\n",
        "  valacc = []\n",
        "  np.random.shuffle(ids)\n",
        "  for i in ids:\n",
        "    if i==None:\n",
        "      continue\n",
        "    start_index = i\n",
        "    image, text_in, text_out = getitem(ids, train=False)\n",
        "\n",
        "    image = image.to(device)\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    outputs = mod(image, text_in, xmask)\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "\n",
        "\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    valacc.append(mask_acc(outputs.detach(), text_out).item())\n",
        "  mod.train(True)\n",
        "  return running_loss/len(ids), np.mean(valacc)\n"
      ],
      "metadata": {
        "id": "rRZ56CuPWl7z"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = torch.load(\"29_4010.pt\", map_location=device) #it is model not dict"
      ],
      "metadata": {
        "id": "6qj0C3t-3xY-"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "access = []\n",
        "val_acc = []\n",
        "optimizer = torch.optim.AdamW(\n",
        "   mod.parameters(),\n",
        "   lr=0.00005)\n",
        "\n",
        "import pylab\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "running_loss = 0\n",
        "for epoch in range(8):\n",
        "  mod.train(True)\n",
        "  ids = [i for i in os.listdir(\"images\") if not i.startswith(\"11203\")]\n",
        "  np.random.shuffle(ids)\n",
        "  for i in range(len(ids)//16):\n",
        "    if i==None:\n",
        "      continue\n",
        "    start_index = i\n",
        "    image, text_in, text_out = getitem(ids, start_index)\n",
        "\n",
        "    image = image.to(device) #mutli process cannot use cuda so moved here\n",
        "    # image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(image, text_in, xmask)\n",
        "    #loss = loss_fn(outputs, text_outi) guaibude yyixiazinamegao\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    losses.append(loss.item())\n",
        "    access.append(mask_acc(outputs.detach(), text_out))\n",
        "\n",
        "  val_acc.append(val()[1])\n",
        "  scheduler.step()\n",
        "  pylab.plot([i.item() for i in access])\n",
        "  pylab.show()\n",
        "  pylab.clf()\n",
        "  print(optimizer.param_groups[0][\"lr\"])"
      ],
      "metadata": {
        "id": "OVUvm_m_cEkb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "16c04485-2885-4b49-d56b-a4d2d66ce520"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-7a8c346aecef>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0maccess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "running_loss = 0\n",
        "for epoch in range(8):\n",
        "  mod.train(True)\n",
        "  ids = [i if not i.startswith(\"11203\") else None for i in os.listdir(\"images\")]\n",
        "  np.random.shuffle(ids)\n",
        "  for i in ids:\n",
        "    if i==None:\n",
        "      continue\n",
        "    start_index = i\n",
        "    image, text_in, text_out = getitem(start_index)\n",
        "\n",
        "    image = image.to(device) #mutli process cannot use cuda so moved here\n",
        "    # image = torch.permute(image, (0, 3, 1, 2))\n",
        "    text_out = pad_pack(text_out)[0].to(device)\n",
        "    padded_x = pad_pack(text_in)\n",
        "\n",
        "    xmask = triangle_mask(padded_x[1]).to(device)\n",
        "    text_in = padded_x[0].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(image, text_in, xmask)\n",
        "    #loss = loss_fn(outputs, text_outi) guaibude yyixiazinamegao\n",
        "    loss = loss_fn(outputs, text_out)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    losses.append(loss.item())\n",
        "    access.append(mask_acc(outputs.detach(), text_out))\n",
        "\n",
        "  val_acc.append(val()[1])\n",
        "  scheduler.step()\n",
        "  pylab.plot([i.item() for i in access])\n",
        "  pylab.show()\n",
        "  pylab.clf()\n",
        "  print(optimizer.param_groups[0][\"lr\"])"
      ],
      "metadata": {
        "id": "Z47Z59bO47fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = np.array([i.item() for i in access[2500:]])"
      ],
      "metadata": {
        "id": "_4BiFGdkrYD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(res==1)/len(res)"
      ],
      "metadata": {
        "id": "Er5kDvoR3jXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pylab.plot([i.item() for i in access])\n",
        "pylab.plot(np.arange(len(val_acc))*(len(access)/len(val_acc)),val_acc)"
      ],
      "metadata": {
        "id": "E7voFa94afTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}