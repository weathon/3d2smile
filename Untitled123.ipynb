{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/Untitled123.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n20yIuGF5afw"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers -U\n",
        "# !pip install tqdm boto3 requests regex sentencepiece sacremoses huggingface_hub\n",
        "# !pip install wandb\n",
        "# !pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktT69Sv4Iki"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from rdkit import Chem\n",
        "from rdkit import DataStructs\n",
        "from rdkit.Chem import AllChem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzzAzcWx4zYw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/home/wg25r/SMRT_dataset.csv\", sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np8VMwEbT2uq"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(\"/home/wg25r/name.csv\")\n",
        "df2 = df2[[\"isosmiles\",\"cid\"]]\n",
        "# df2.to_csv(\"name.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17AMumCMSiGn"
      },
      "outputs": [],
      "source": [
        "with open(\"pid\",\"w\") as f:\n",
        "  f.write(\"\\n\".join([str(i) for i in list(df[\"pubchem\"])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnCL7WCW5FnM",
        "outputId": "9a2caab8-0914-4497-f2da-731711871841"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "smiles_encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23isgOJgj_H6"
      },
      "outputs": [],
      "source": [
        "smiles_encoder.encoder.layer=smiles_encoder.encoder.layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfaEbl2o7jAv"
      },
      "outputs": [],
      "source": [
        "# smiles_arr = list(df2[\"isosmiles\"][df[\"pubchem\"]])\n",
        "smiles_arr = []\n",
        "for i in list(df[\"pubchem\"]):\n",
        "  smiles_arr.append(df2[df2[\"cid\"]==i][\"isosmiles\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfIjoXqwWsYO"
      },
      "outputs": [],
      "source": [
        "smiles_arr = [i.item() if len(i)==1 else \"\" for i in smiles_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I_gjHemSJAvL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "fpgen = AllChem.GetRDKitFPGenerator()\n",
        "fps = []\n",
        "for i in smiles_arr:\n",
        "  a = (fpgen.GetFingerprint(Chem.MolFromSmiles(i)))\n",
        "  fps.append(np.array(a))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pi6NiKda0FA1"
      },
      "outputs": [],
      "source": [
        "del smiles_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y-8irptkCTGP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wVpO2M4X-AU7"
      },
      "outputs": [],
      "source": [
        "padding = len(a)+1\n",
        "CLS = len(a)+2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MZIPysv8ViJr"
      },
      "outputs": [],
      "source": [
        "fps = [np.where(i==1) for i in fps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u31DBBKCAJzt"
      },
      "outputs": [],
      "source": [
        "# https://youtu.be/ug8YvZOjOCE?t=2692\n",
        "class CL(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embeding = torch.nn.Embedding(len(a)+2, 512, padding)\n",
        "    self.transformers = torch.nn.Sequential(\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "        torch.nn.TransformerEncoderLayer(512, 8, batch_first=True),\n",
        "    )\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(512, 64),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(64, 32),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(32, 1),\n",
        "        torch.nn.ReLU(),\n",
        "    )\n",
        "  def forward(self, smiles):\n",
        "    return self.mlp(self.transformers(self.embeding(smiles))[:,0,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h7JAFYcfLLZM",
        "outputId": "89a78972-4106-4f62-e292-b4d4e0509b43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwguo6358\u001b[0m (\u001b[33m3dsmile\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pB10KKOfLYhk",
        "outputId": "bc7a4963-10b2-46da-cac2-792e8c604d63"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231226_014444-gc10wlkq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/3dsmile/my-awesome-project/runs/gc10wlkq' target=\"_blank\">lively-shadow-68</a></strong> to <a href='https://wandb.ai/3dsmile/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/3dsmile/my-awesome-project' target=\"_blank\">https://wandb.ai/3dsmile/my-awesome-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/3dsmile/my-awesome-project/runs/gc10wlkq' target=\"_blank\">https://wandb.ai/3dsmile/my-awesome-project/runs/gc10wlkq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    project=\"my-awesome-project\",\n",
        "    config={\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y6syoFB_L_E2",
        "outputId": "d7a621b4-0766-44ec-9fcf-b3af69433622"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-fb74bd9416da>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  fps_val = np.array(fps)[-100:]\n",
            "<ipython-input-18-fb74bd9416da>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  fps = np.array(fps)[:-100]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "model = CL().to(device)\n",
        "df[\"rt\"] = df[\"rt\"]/(max(df[\"rt\"]))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00003)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
        "\n",
        "fps_val = np.array(fps)[-100:]\n",
        "RT_val = np.array(list(df[\"rt\"]))[-100:]\n",
        "\n",
        "fps = np.array(fps)[:-100]\n",
        "RT = np.array(list(df[\"rt\"]))[:-100]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "slices = list(range(0, (len(RT)), BATCH_SIZE))\n",
        "slices.append(-1)\n",
        "\n",
        "slices_val = list(range(0, (len(RT_val)), BATCH_SIZE))\n",
        "slices_val.append(-1)\n",
        "\n",
        "losses = []\n",
        "val_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ElDz206hCEJ"
      },
      "outputs": [],
      "source": [
        "fps = [i[0] for i in fps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_dIF6bWzc9c-",
        "outputId": "3ae7fe06-7a99-4f41-a79f-5971768a7eaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-c063447c0de5>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  fps = np.array(fps)\n"
          ]
        }
      ],
      "source": [
        "fps = np.array(fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ajka9kIZnhzQ"
      },
      "outputs": [],
      "source": [
        "def pad(strs):\n",
        "  # print(strs)\n",
        "  maxlen = max([len(i) for i in strs])+1\n",
        "  strs = [[CLS]+list(i) for i in strs]\n",
        "  for i in range(len(strs)):\n",
        "    strs[i] += [padding] * (maxlen - len(strs[i]))\n",
        "  return np.array(list(strs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad([[1,2,3],[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn-GWzqC0zdH",
        "outputId": "853bf52d-9a4b-4231-c8db-627841a7720f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2050,    1,    2,    3],\n",
              "       [2050,    1, 2049, 2049]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "8apVU_YwGB5w",
        "outputId": "7dc65317-9e00-4db2-efab-f5e25fd31e81"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8f261d254a90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(30):\n",
        "  indices = np.arange(len(RT))\n",
        "  np.random.shuffle(indices)\n",
        "  fps = fps[indices]\n",
        "  RT = RT[indices]\n",
        "  running_loss = 0\n",
        "  for i in range(len(slices)-1):\n",
        "    X_1 = torch.tensor(pad(fps[slices[i]:slices[i+1]])).to(device)\n",
        "    Y = torch.tensor(RT[slices[i]:slices[i+1]]).to(device).unsqueeze(-1).to(torch.float32)\n",
        "    optimizer.zero_grad()\n",
        "    predicted = model(X_1)\n",
        "    loss = torch.nn.functional.mse_loss(predicted, Y)\n",
        "    # losses.append(loss.cpu())\n",
        "    running_loss+=loss.cpu()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i%30==30-1:\n",
        "      print(\"new lr\")\n",
        "      wandb.log({\"loss\":running_loss/30,  \"lr\":optimizer.param_groups[0][\"lr\"]})\n",
        "      scheduler.step()\n",
        "      running_loss=0\n",
        "\n",
        "  # validation\n",
        "  running_loss = []\n",
        "  for i in range(len(slices_val)-1):\n",
        "    X_1 = torch.tensor(pad(fps_val[slices_val[i]:slices_val[i+1]])).to(device)\n",
        "    Y = torch.tensor(RT_val[slices_val[i]:slices_val[i+1]]).to(device).unsqueeze(-1).to(torch.float32)\n",
        "    predicted = model(X_1)\n",
        "    loss = torch.nn.functional.mse_loss(predicted, Y)\n",
        "    running_loss.append(loss.cpu().item())\n",
        "  val_loss.append(np.mean(running_loss))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPicQfoK/oZQu1sCoH28tak",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}