{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMs7mr/RKBKVI5jT+4tXw/d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weathon/3d2smile/blob/main/ITM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yx9l03x7a-vq"
      },
      "outputs": [],
      "source": [
        "# !pip install rdkit deepsmiles\n",
        "# !pip3 install torchinfo\n",
        "# !pip install tqdm boto3 requests regex sentencepiece sacremoses huggingface_hub\n",
        "# !wget http://file.weasoft.com/80k.csv\n",
        "# !pip install transformers -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "import deepsmiles\n",
        "import numpy as np\n",
        "import pylab\n",
        "converter = deepsmiles.Converter(rings=True, branches=True)\n",
        "def deepsmiles_to_img(ds):\n",
        "   img = np.array(Draw.MolToImage(Chem.MolFromSmiles(converter.decode(ds)), size=(400,400)).convert(\"L\", dither=None).convert(\"RGB\"))\n",
        "   img = np.where(img<253, 0, 1) * img\n",
        "   return img\n",
        "\n",
        "def smiles_to_img(smiles):\n",
        "  return deepsmiles_to_img(converter.encode(smiles))\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "pjjuor65bCnW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "csv = pandas.read_csv(\"80k.csv\")\n",
        "\n",
        "smiles_arr = []\n",
        "for smiles in csv['canonicalsmiles']:\n",
        "  smiles_arr.append(converter.encode(smiles))"
      ],
      "metadata": {
        "id": "y8M7a7upbE11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')"
      ],
      "metadata": {
        "id": "dYz9P5RHbbBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "image_encoder = torchvision.models.swin_s(weights='DEFAULT')"
      ],
      "metadata": {
        "id": "crK6sA47cbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchinfo\n",
        "image_encoder.norm = torch.nn.Identity()\n",
        "image_encoder.permute = torch.nn.Identity()\n",
        "image_encoder.avgpool = torch.nn.Identity()\n",
        "image_encoder.flatten = torch.nn.Flatten(-3, -2)\n",
        "\n",
        "image_encoder.head = torch.nn.Identity()\n",
        "# torchinfo.summary(image_encoder, input_size=(1, 3, 400, 400))"
      ],
      "metadata": {
        "id": "apZrQET3cjEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder(torch.rand(1, 3, 400, 400)).shape"
      ],
      "metadata": {
        "id": "GJ6-GN65fXRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_encoder.pooler = torch.nn.Identity()\n",
        "# torchinfo.summary(smiles_encoder, input_data=torch.ones(1, 128, dtype=torch.int32))"
      ],
      "metadata": {
        "id": "sebmyOu_eASp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_encoder(torch.zeros(1,128, dtype=torch.int32)).last_hidden_state.shape"
      ],
      "metadata": {
        "id": "V-IBYvN4fdY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(\"\".join(smiles_arr)))\n",
        "tokens = dict.fromkeys(chars)\n",
        "for i, char in enumerate(chars):\n",
        "  tokens[char] = i\n",
        "reversed_mapping = {}\n",
        "for i, char in enumerate(chars):\n",
        "  reversed_mapping[i] = char\n",
        "for i, smiles in enumerate(smiles_arr):\n",
        "  smiles_arr[i] = [tokens[char] for char in smiles]"
      ],
      "metadata": {
        "id": "VjcR4BCQbjNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://youtu.be/ug8YvZOjOCE?t=2692\n",
        "class CL(torch.nn.Module):\n",
        "  def __init__(self, maxlen):\n",
        "    super().__init__()\n",
        "    self.image_encoder = image_encoder.to(device)\n",
        "    self.smiles_encoder = smiles_encoder.to(device)\n",
        "    self.smiles_proj = torch.nn.Linear(768, 512).to(device)\n",
        "    self.pos1 = torch.nn.Embedding(13*13, 768).to(device)\n",
        "    self.pos2 = torch.nn.Embedding(maxlen, 768).to(device)\n",
        "    self.modal = torch.nn.Embedding(2, 768).to(device)\n",
        "    self.i_begin = torch.nn.Embedding(1, 768).to(device)\n",
        "    self.t_begin = torch.nn.Embedding(1, 768).to(device)\n",
        "    self.fusion = torch.nn.Sequential(\n",
        "        torch.nn.TransformerEncoderLayer(d_model=768, nhead=8, dim_feedforward=512, dropout=0.1, batch_first=True).to(device),\n",
        "        torch.nn.TransformerEncoderLayer(d_model=768, nhead=8, dim_feedforward=512, dropout=0.1, batch_first=True).to(device),\n",
        "        torch.nn.TransformerEncoderLayer(d_model=768, nhead=8, dim_feedforward=512, dropout=0.1, batch_first=True).to(device),\n",
        "        torch.nn.TransformerEncoderLayer(d_model=768, nhead=8, dim_feedforward=512, dropout=0.1, batch_first=True).to(device),\n",
        "    ).to(device)\n",
        "    self.head = torch.nn.Linear(768, 1).to(device)\n",
        "  def forward(self, image, smiles):\n",
        "    # print(self.i_begin(torch.tensor(0)).unsqueeze(0).unsqueeze(0).repeat(image.shape[0],1,1).shape)\n",
        "    image_embedding = self.image_encoder(image)\n",
        "    smiles_embedding = self.smiles_encoder(smiles, attention_mask=(smiles!=30)).last_hidden_state\n",
        "    pos_image = self.pos1(torch.arange(13*13).to(device))\n",
        "    m_i = self.modal(torch.zeros(image_embedding.shape[1], dtype=torch.int32).to(device))\n",
        "    image_embedding = image_embedding + pos_image + m_i\n",
        "    pos_txt = self.pos2(torch.arange(smiles_embedding.shape[1]).to(device))\n",
        "    m_t = self.modal(torch.ones(smiles_embedding.shape[1], dtype=torch.int32).to(device))\n",
        "    smiles_embedding = smiles_embedding + pos_txt + m_t\n",
        "    seq = torch.cat([self.i_begin(torch.tensor(0).to(device)).unsqueeze(0).unsqueeze(0).repeat(image.shape[0],1,1), image_embedding, self.i_begin(torch.tensor(0).to(device)).unsqueeze(0).unsqueeze(0).repeat(image.shape[0],1,1), smiles_embedding], dim=1)\n",
        "    src_mask = torch.cat([torch.ones(image.shape[0], image_embedding.shape[1]+2).to(device),(smiles!=30)], dim=-1)\n",
        "    for i in self.fusion:\n",
        "      seq = i(seq, src_key_padding_mask =src_mask) #src_mask should be the key mask duzikuneyunedkoukekun\n",
        "    return self.head(seq[:,0,:])\n",
        "# CL(512)(torch.rand(2, 3, 400, 400), torch.zeros(2,128, dtype=torch.int32))"
      ],
      "metadata": {
        "id": "eE41myMyfKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reversed_mapping[29] = \"\"\n",
        "reversed_mapping[30] = \"\""
      ],
      "metadata": {
        "id": "XGu1I4aHwC8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "model = CL(max([len(i) for i in smiles_arr])).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99987)\n",
        "\n",
        "n = 4\n",
        "for EPOCH in range(30):\n",
        "  for i in range(0, len(smiles_arr), n):\n",
        "    images_list = []\n",
        "    smiles_list = []\n",
        "    maxlen = 0\n",
        "    ans = []\n",
        "    for j in range(n):\n",
        "      if random.random()>0.5: #touyunskaok\n",
        "          image = deepsmiles_to_img(\"\".join([reversed_mapping[_] for _ in smiles_arr[i+j]]))\n",
        "          image = torch.tensor(image).permute(2,0,1)\n",
        "          images_list.append(image)\n",
        "          smiles_list.append(smiles_arr[i+j])\n",
        "          maxlen = max(maxlen, len(smiles_arr[i+j]))\n",
        "          ans.append(1)\n",
        "      else:\n",
        "          image = deepsmiles_to_img(\"\".join([reversed_mapping[_] for _ in smiles_arr[i+j]]))\n",
        "          image = torch.tensor(image).permute(2,0,1)\n",
        "          images_list.append(image)\n",
        "          id = random.randint(0, len(smiles_arr)-1)\n",
        "          smiles_list.append(smiles_arr[id])\n",
        "          maxlen = max(maxlen, len(smiles_arr[id]))\n",
        "          ans.append(0)\n",
        "    for j in range(n):\n",
        "      smiles_list[j] += [30] * (maxlen - len(smiles_list[j]))\n",
        "\n",
        "    images_list = torch.stack(images_list).to(torch.float32)\n",
        "    smiles_list = torch.tensor(smiles_list, dtype=torch.int32)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(images_list.to(device), smiles_list.to(device))\n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, torch.tensor(ans).unsqueeze(1).to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)"
      ],
      "metadata": {
        "id": "BmA7hk-dpFr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}